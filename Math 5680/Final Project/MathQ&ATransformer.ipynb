{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Saw43LjtY78K"
      },
      "source": [
        "# Math Q&A Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "module_path = os.path.abspath('.')\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESQ9D1ZPZDf1"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDvOOvZ-XdOn"
      },
      "source": [
        "Summarize dataset structure for the `arithmetic` category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHDaUJUTSglp",
        "outputId": "ff06fb9a-31e7-46ff-ce4b-3f101f4f0c69"
      },
      "outputs": [],
      "source": [
        "from math_dataset import MathDataset\n",
        "\n",
        "for data_type, categories in MathDataset.subcategories().items():\n",
        "  print(f'Data Type: {data_type}')\n",
        "  for subcat in categories['arithmetic']:\n",
        "    print(f'  {subcat}')\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_W8QrwlYK60"
      },
      "source": [
        "Print some example questions and answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy3v679TXsSV",
        "outputId": "ab755475-ea10-4b74-85b6-5bbb239e7cee"
      },
      "outputs": [],
      "source": [
        "test_dataset = MathDataset('train-easy', 'arithmetic', 'add_or_sub')\n",
        "\n",
        "for question, answer in zip(*test_dataset[5000:5005]):\n",
        "  print(f'Question: {question}')\n",
        "  print(f'Answer: {answer}')\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7hziDg2xdn7"
      },
      "source": [
        "## Setup for Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd0ZiueUx31A"
      },
      "source": [
        "Determine token set for arithmetic training easy. Takes ~30s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAAncYCLx3oG",
        "outputId": "9e43e835-5829-461e-bc5b-eff0f4ace74d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import utils\n",
        "\n",
        "# load all arithmetic training easy subcategories\n",
        "arthimetic_easy_subcats = MathDataset.subcategories()['train-easy']['arithmetic']\n",
        "datasets = [\n",
        "    MathDataset('train-easy', 'arithmetic', s)\n",
        "    for s in arthimetic_easy_subcats\n",
        "]\n",
        "\n",
        "# put together all subcategories\n",
        "arithmetic_easy = torch.utils.data.ConcatDataset(datasets)\n",
        "\n",
        "# find all possible tokens\n",
        "arithmetic_easy_tokens = utils.token_set(arithmetic_easy)\n",
        "\n",
        "arithmetic_easy_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwq9wxIB5lvA"
      },
      "source": [
        "Evaluate input-output sequence lengths. Takes ~1min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "id": "tfmprD4E5lBI",
        "outputId": "b69f5521-e092-422d-d11b-e8fa1ffb4f88"
      },
      "outputs": [],
      "source": [
        "utils.plot_length_histogram(arithmetic_easy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llb32_RELpMa"
      },
      "source": [
        "## Exploring Token Embeddings and Attention Masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfiUh707Lp_6"
      },
      "outputs": [],
      "source": [
        "from transformer import Transformer, TokenEmbedding\n",
        "\n",
        "transformer = Transformer(\n",
        "    TokenEmbedding(arithmetic_easy_tokens, 6),  # 6 = d_model\n",
        "    30,   # = max_output_length\n",
        "    5,    # = n_encoder_layers\n",
        "    5,    # = n_decoder_layers\n",
        "    1,    # = n_heads\n",
        "    1024  # = d_ff\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KRegsZ1Pf3v",
        "outputId": "f1b89bbd-713e-4fd5-dc12-01fda21e38eb"
      },
      "outputs": [],
      "source": [
        "sample_questions = [arithmetic_easy[i][0] for i in range(5)]\n",
        "sample_questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixuSqIznOtU-",
        "outputId": "8f5ca9a3-6f94-4241-be7e-fd0a5b673383"
      },
      "outputs": [],
      "source": [
        "sample_indices = transformer.token_embedding.indices(sample_questions)\n",
        "sample_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZKdL-1kTGry",
        "outputId": "800129f0-8bf9-45f1-906c-a6f73b135715"
      },
      "outputs": [],
      "source": [
        "sample_unembedded = transformer.token_embedding.unembed(sample_indices)\n",
        "sample_unembedded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YBz97qNTOES",
        "outputId": "ab44af5c-df47-466f-dea6-6f2e5a6eae4a"
      },
      "outputs": [],
      "source": [
        "sample_unembedded_special = transformer.token_embedding.unembed(\n",
        "    sample_indices, include_special=True\n",
        ")\n",
        "sample_unembedded_special"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oswg-yHLQaL9",
        "outputId": "4043d7e4-5bf7-459d-8d52-d82ea798a3a4"
      },
      "outputs": [],
      "source": [
        "sample_token_embeddings = transformer.token_embedding(sample_indices)\n",
        "sample_token_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2_7C5k4RfCk",
        "outputId": "17737571-b840-4ded-9cbc-11c210302be0"
      },
      "outputs": [],
      "source": [
        "def print_full(matrix, digits='.02f'):\n",
        "  for r in range(matrix.shape[0]):\n",
        "    for c in range(matrix.shape[1]):\n",
        "      print(f'{float(matrix[r, c]):{digits}}', end=' ')\n",
        "    print()\n",
        "  print()\n",
        "\n",
        "\n",
        "mask = transformer.input_attention_mask(sample_indices)\n",
        "for index_seq, mask in zip(sample_indices, mask):\n",
        "  print('Token sequence indices and input mask matrix')\n",
        "  print_full(torch.cat([index_seq[None], mask]), digits='g')\n",
        "\n",
        "  print('Post-softmax')\n",
        "  print_full(torch.nn.functional.softmax(mask, dim=1), digits='.4f')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Mi4DIcgSaHG"
      },
      "outputs": [],
      "source": [
        "sample_outputs = [arithmetic_easy[i][1] for i in range(5)]\n",
        "sample_output_indices = transformer.token_embedding.indices(sample_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmESJ0IYW7oO",
        "outputId": "ee4c89e6-b482-426a-a3d0-fe9311e55ed3"
      },
      "outputs": [],
      "source": [
        "sample_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fksug2sLW9IJ",
        "outputId": "a5b05068-cf62-4667-a4d4-3eb3b63d319b"
      },
      "outputs": [],
      "source": [
        "sample_output_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ubq0_CoW-tj",
        "outputId": "acacb810-b2b9-4ade-ac96-78cd5d23982f"
      },
      "outputs": [],
      "source": [
        "mask = transformer.output_attention_mask(sample_output_indices)\n",
        "for index_seq, mask in zip(sample_output_indices, mask):\n",
        "  print('Token sequence indices and output self-attention mask matrix')\n",
        "  print_full(torch.cat([index_seq[None], mask]), digits='g')\n",
        "\n",
        "  print('Post-softmax')\n",
        "  print_full(torch.nn.functional.softmax(mask, dim=1), digits='.4f')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdSlMn0WXJdG",
        "outputId": "df78f36c-9515-4fb9-e91e-67defb3b9a63"
      },
      "outputs": [],
      "source": [
        "mask = transformer.cross_attention_mask(sample_indices, sample_output_indices)\n",
        "for index_seq, mask in zip(sample_indices, mask):\n",
        "  print('Token sequence indices and output cross-attention mask matrix')\n",
        "  print_full(torch.cat([index_seq[None], mask]), digits='g')\n",
        "\n",
        "  print('Post-softmax')\n",
        "  print_full(torch.nn.functional.softmax(mask, dim=1), digits='.4f')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9PsqWbSpIpV"
      },
      "source": [
        "## Transformer Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TxlYxEyUosL"
      },
      "outputs": [],
      "source": [
        "from training import QATransformerTrainer, SavePeriodicallyCallback\n",
        "\n",
        "\n",
        "model = Transformer(\n",
        "    TokenEmbedding(arithmetic_easy_tokens, 512),\n",
        "    30, 6, 6, 8, 2048, p_dropout=0.1\n",
        ").cuda()\n",
        "\n",
        "ar_easy_dl = torch.utils.data.DataLoader(\n",
        "    arithmetic_easy, batch_size=128, shuffle=True,\n",
        ")\n",
        "\n",
        "optim = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=6e-4,\n",
        "    betas=(.9, .995),\n",
        "    eps=1e-9\n",
        ")\n",
        "\n",
        "cel = torch.nn.CrossEntropyLoss(\n",
        "    ignore_index=model.token_embedding.pad_index, label_smoothing=.05\n",
        ")\n",
        "\n",
        "def loss_fn(prob, actual):\n",
        "    pad_b, pad_n = torch.nonzero(actual == model.token_embedding.pad_index, as_tuple=True)\n",
        "    prob[pad_b, :, pad_n] = -1000.\n",
        "    return cel(prob, actual)\n",
        "\n",
        "trainer = QATransformerTrainer('model3', model, ar_easy_dl, optim, loss_fn, 100)\n",
        "\n",
        "# save every 900s = 15min\n",
        "save_callback = SavePeriodicallyCallback(trainer, 900)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pT7ZOr5GUqRB",
        "outputId": "080f75b1-7e9e-434d-a394-2cbcc2f4dd79"
      },
      "outputs": [],
      "source": [
        "losses, accuracies = trainer.train(\n",
        "    epochs=2,\n",
        "    batch_callbacks=[save_callback],\n",
        "    verbosity=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from simple_dataset import SimpleDataset1\n",
        "\n",
        "simple_dataset = SimpleDataset1(1000000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from training import QATransformerTrainer, SavePeriodicallyCallback\n",
        "\n",
        "model = Transformer(\n",
        "    TokenEmbedding(SimpleDataset1.tokens(), 256),\n",
        "    130, 6, 6, 8, 1024, p_dropout=0.1\n",
        ").cuda()\n",
        "\n",
        "simple_dl = torch.utils.data.DataLoader(\n",
        "    simple_dataset, batch_size=256, shuffle=True,\n",
        ")\n",
        "\n",
        "optim = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=1e-5,\n",
        "    betas=(.9, .995),\n",
        "    eps=1e-9\n",
        ")\n",
        "\n",
        "cel = torch.nn.CrossEntropyLoss(\n",
        "    ignore_index=model.token_embedding.pad_index, label_smoothing=.05\n",
        ")\n",
        "\n",
        "def loss_fn(prob, actual):\n",
        "    pad_b, pad_n = torch.nonzero(actual == model.token_embedding.pad_index, as_tuple=True)\n",
        "    prob[pad_b, :, pad_n] = -1000.\n",
        "    return cel(prob, actual)\n",
        "\n",
        "trainer = QATransformerTrainer('model3', model, simple_dl, optim, loss_fn, 100)\n",
        "\n",
        "# save every 900s = 15min\n",
        "save_callback = SavePeriodicallyCallback(trainer, 900)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "losses, accuracies = trainer.train(\n",
        "    epochs=2,\n",
        "    batch_callbacks=[save_callback],\n",
        "    verbosity=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch = next(iter(simple_dl))\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for q, a in zip(*batch):\n",
        "        print(q)\n",
        "        print(a)\n",
        "        print(model(q))\n",
        "        print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
