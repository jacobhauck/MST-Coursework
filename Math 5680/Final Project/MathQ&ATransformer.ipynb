{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Saw43LjtY78K"
      },
      "source": [
        "# Math Q&A Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a9mjq_NY_L1"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoKbO3SvPfj6"
      },
      "source": [
        "Global variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Os2z2QgNPhA0"
      },
      "outputs": [],
      "source": [
        "DATASET_FOLDER = 'mathematics_dataset'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESQ9D1ZPZDf1"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTAlD9L-QVX7"
      },
      "source": [
        "Class for loading data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YlhajsdsQZNr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "\n",
        "class MathDataset(torch.utils.data.Dataset):\n",
        "  \"\"\"\n",
        "  Represents a particular sub-dataset within the overall Google Math\n",
        "  Dataset\n",
        "  \"\"\"\n",
        "\n",
        "  @staticmethod\n",
        "  def root():\n",
        "    \"\"\"\n",
        "    :return: root directory of the dataset\n",
        "    \"\"\"\n",
        "    return os.path.join(DATASET_FOLDER, 'mathematics_dataset-v1.0')\n",
        "\n",
        "  @staticmethod\n",
        "  def data_types():\n",
        "    \"\"\"\n",
        "    :return: data types in the dataset (train-easy, train-hard,\n",
        "    eval, etc.)\n",
        "    \"\"\"\n",
        "    return tuple(\n",
        "      d for d in os.listdir(MathDataset.root())\n",
        "      if os.path.isdir(os.path.join(MathDataset.root(), d))\n",
        "    )\n",
        "\n",
        "  @staticmethod\n",
        "  def categories():\n",
        "    \"\"\"\n",
        "    :return: dictionary of data type -> list of categories for each\n",
        "    data type in the dataset\n",
        "    \"\"\"\n",
        "    categories = {}\n",
        "    for data_type in MathDataset.data_types():\n",
        "      data_type_cats = []\n",
        "      dtype_folder = os.path.join(MathDataset.root(), data_type)\n",
        "      for file_name in os.listdir(dtype_folder):\n",
        "        data_type_cats.append(file_name[:file_name.find('__')])\n",
        "      categories[data_type] = tuple(data_type_cats)\n",
        "\n",
        "    return categories\n",
        "\n",
        "  @staticmethod\n",
        "  def subcategories():\n",
        "    \"\"\"\n",
        "    :return: dictionary of data type -> category -> list of subcategories\n",
        "    for each category in each data type of the dataset\n",
        "    \"\"\"\n",
        "    subcategories = {}\n",
        "    for data_type in MathDataset.data_types():\n",
        "      data_type_subcats = {}\n",
        "      dtype_folder = os.path.join(MathDataset.root(), data_type)\n",
        "      for file_name in os.listdir(dtype_folder):\n",
        "        i_du = file_name.find('__')\n",
        "        cat, subcat = file_name[:i_du], file_name[i_du + 2 : -4]\n",
        "\n",
        "        if cat not in data_type_subcats:\n",
        "          data_type_subcats[cat] = []\n",
        "        data_type_subcats[cat].append(subcat)\n",
        "\n",
        "      subcategories[data_type] = data_type_subcats\n",
        "\n",
        "    return subcategories\n",
        "\n",
        "  def __init__(self, data_type, category, subcategory):\n",
        "    \"\"\"\n",
        "    Load and initialize the subcategory dataset within the given category\n",
        "    and data type\n",
        "    :param data_type: the data type to load from. Must be one of\n",
        "    MathDataset.data_types()\n",
        "    :param category: the category to load from. Must be one of\n",
        "    MathDataset.categories()[data_type]\n",
        "    :param subcategory: the subcategory dataset to load. Must be one of\n",
        "    MathDataset.subcategories()[data_type][category]\n",
        "    \"\"\"\n",
        "    self.data_type = data_type\n",
        "    self.category = category\n",
        "    self.subcategory = subcategory\n",
        "\n",
        "    with open(self.path, 'r') as f:\n",
        "      lines = f.read().splitlines()\n",
        "\n",
        "    self.questions = lines[::2]\n",
        "    self.answers = lines[1::2]\n",
        "\n",
        "  @property\n",
        "  def path(self):\n",
        "    \"\"\"\n",
        "    :return: Path to the subcategory folder\n",
        "    \"\"\"\n",
        "    folder = os.path.join(MathDataset.root(), self.data_type)\n",
        "    file_name = f'{self.category}__{self.subcategory}.txt'\n",
        "    return os.path.join(folder, file_name)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    \"\"\"\n",
        "    :param index: index of question-answer pair in range(len(self))\n",
        "    :return: The question answer pair in the dataset at the given index\n",
        "    \"\"\"\n",
        "    return self.questions[index], self.answers[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"\n",
        "    :return: How many question-answer pairs are in the dataset\n",
        "    \"\"\"\n",
        "    return len(self.questions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDvOOvZ-XdOn"
      },
      "source": [
        "Summarize dataset structure for the `arithmetic` category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHDaUJUTSglp",
        "outputId": "ff06fb9a-31e7-46ff-ce4b-3f101f4f0c69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Type: extrapolate\n",
            "  add_or_sub_big\n",
            "  add_sub_multiple_longer\n",
            "  div_big\n",
            "  mixed_longer\n",
            "  mul_big\n",
            "  mul_div_multiple_longer\n",
            "\n",
            "Data Type: interpolate\n",
            "  add_or_sub\n",
            "  add_or_sub_in_base\n",
            "  add_sub_multiple\n",
            "  div\n",
            "  mixed\n",
            "  mul\n",
            "  mul_div_multiple\n",
            "  nearest_integer_root\n",
            "  simplify_surd\n",
            "\n",
            "Data Type: train-easy\n",
            "  add_or_sub\n",
            "  add_or_sub_in_base\n",
            "  add_sub_multiple\n",
            "  div\n",
            "  mixed\n",
            "  mul\n",
            "  mul_div_multiple\n",
            "  nearest_integer_root\n",
            "  simplify_surd\n",
            "\n",
            "Data Type: train-hard\n",
            "  add_or_sub\n",
            "  add_or_sub_in_base\n",
            "  add_sub_multiple\n",
            "  div\n",
            "  mixed\n",
            "  mul\n",
            "  mul_div_multiple\n",
            "  nearest_integer_root\n",
            "  simplify_surd\n",
            "\n",
            "Data Type: train-medium\n",
            "  add_or_sub\n",
            "  add_or_sub_in_base\n",
            "  add_sub_multiple\n",
            "  div\n",
            "  mixed\n",
            "  mul\n",
            "  mul_div_multiple\n",
            "  nearest_integer_root\n",
            "  simplify_surd\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for data_type, categories in MathDataset.subcategories().items():\n",
        "  print(f'Data Type: {data_type}')\n",
        "  for subcat in categories['arithmetic']:\n",
        "    print(f'  {subcat}')\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_W8QrwlYK60"
      },
      "source": [
        "Print some example questions and answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy3v679TXsSV",
        "outputId": "ab755475-ea10-4b74-85b6-5bbb239e7cee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is -15845 minus -35?\n",
            "Answer: -15810\n",
            "\n",
            "Question: Work out 0.95 - 2721.\n",
            "Answer: -2720.05\n",
            "\n",
            "Question: Sum -20.4 and -0.024.\n",
            "Answer: -20.424\n",
            "\n",
            "Question: Calculate -1.4968 + 6.\n",
            "Answer: 4.5032\n",
            "\n",
            "Question: What is -0.3 less than 14240?\n",
            "Answer: 14240.3\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_dataset = MathDataset('train-easy', 'arithmetic', 'add_or_sub')\n",
        "\n",
        "for question, answer in zip(*test_dataset[5000:5005]):\n",
        "  print(f'Question: {question}')\n",
        "  print(f'Answer: {answer}')\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FMSKvUVZHd8"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDA5Vr16YbJz"
      },
      "source": [
        "Modules needed to implement Transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ASP-aLe_3XVS"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "import torch\n",
        "\n",
        "\n",
        "class TokenEmbedding(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Manages the conversion between three different token-string\n",
        "  representations:\n",
        "\n",
        "  1. String (human-readable) representation, e.g., 'What is 3 + 4?'\n",
        "\n",
        "  2. Index (computer-readable) representation, e.g.,\n",
        "  [start_index, i('W'), i('h'), i('a'), ..., i(' '), i('?'), end_index],\n",
        "  where i is a function that maps a string representation to an integral\n",
        "  index.\n",
        "\n",
        "  3. Vector (ML embedding) representation, e.g.,\n",
        "  [t(i1), t(i2), ... t(in)], where i1, i2, ..., in are the integral\n",
        "  indices of the tokens, and t(i) is a vector in R^d for every index i\n",
        "\n",
        "  - Use TokenEmbedding.indices to convert 1. -> 2.\n",
        "  - Use TokenEmbedding.unembed to convert 2. -> 1.\n",
        "  - Use TokenEmbedding.forward to convert 2. -> 3.\n",
        "\n",
        "  The model decides how to convert 3. -> 2.\n",
        "  \"\"\"\n",
        "\n",
        "  # Indices used for special tokens that are always included\n",
        "  # in a token set\n",
        "  start_index = 0\n",
        "  end_index = 1\n",
        "  pad_index = 2\n",
        "\n",
        "  def __init__(self, tokens: List[str], d: int):\n",
        "    \"\"\"\n",
        "    :param tokens: A list of tokens (as strings)\n",
        "    :param d: Embedding dimension\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.d = d\n",
        "\n",
        "    # <START> token is always 0, <END> token is always 1,\n",
        "    # <PAD> token is always 2\n",
        "    self.tokens = ('<START>', '<END>', '<PAD>') + tuple(tokens)\n",
        "\n",
        "    # build mapping from token value to token index\n",
        "    self.index = {t: i for i, t in enumerate(self.tokens)}\n",
        "\n",
        "    # Store embedding matrix as a Parameter so that it can be\n",
        "    # learned during training\n",
        "    self.embedding_matrix = torch.nn.Parameter(\n",
        "      torch.empty((self.num_tokens, d))\n",
        "    )\n",
        "\n",
        "    # I found it difficult to find suggestions on how to initialize\n",
        "    # the embedding matrix, but PyTorch just samples from N(0,I)\n",
        "    # (c.f. torch.nn.Embedding), so we will do this as well\n",
        "    torch.nn.init.normal_(self.embedding_matrix)\n",
        "\n",
        "  @property\n",
        "  def num_tokens(self):\n",
        "    return len(self.tokens)\n",
        "\n",
        "  def indices(self, x: List[str]):\n",
        "    \"\"\"\n",
        "    :param x: list of B strings of tokens, each of length n_i\n",
        "    :return: tensor (B, n) of B sets of n token indices, where\n",
        "    n = 2 + max_i n_i. Sequences are formed by prepending <start>\n",
        "    tokens and appending <EOS> tokens. Sequences shorter than n tokens\n",
        "    in length are padded with <pad> tokens\n",
        "    \"\"\"\n",
        "    unpadded = [[self.index[t.lower()] for t in s] for s in x]\n",
        "    n = max(map(len, unpadded))\n",
        "\n",
        "    # pad sequences with <START>, <END> and <PAD> tokens\n",
        "    padded = [\n",
        "        [self.start_index]\n",
        "        + s\n",
        "        + [self.end_index]\n",
        "        + [self.pad_index] * (n - len(s))\n",
        "        for s in unpadded\n",
        "    ]\n",
        "\n",
        "    return torch.tensor(\n",
        "        padded, dtype=torch.long, device=self.embedding_matrix.device\n",
        "    )\n",
        "\n",
        "  def unembed(self, indices, include_special: bool=False):\n",
        "    \"\"\"\n",
        "    :param indices: (B, n) tensor B sets of n token indices\n",
        "    :param include_special: Whether to include the special <START>, <END>\n",
        "    and <PAD> tokens\n",
        "    :return: the string corresponding to the given token indices\n",
        "    \"\"\"\n",
        "    if include_special:\n",
        "      return [''.join(self.tokens[i] for i in si) for si in indices]\n",
        "    else:\n",
        "      return [\n",
        "          ''.join(\n",
        "              self.tokens[i]\n",
        "              for i in si[1 : si.tolist().index(self.end_index)]\n",
        "          )\n",
        "          for si in indices\n",
        "      ]\n",
        "\n",
        "  def forward(self, indices):\n",
        "    \"\"\"\n",
        "    :param indices: (B, n) tensor of B sequences of n token indices\n",
        "    :return: (B, n, d) tensor of B sets of embedded tokens\n",
        "    \"\"\"\n",
        "    unscaled = self.embedding_matrix[indices, :]\n",
        "\n",
        "    # Vaswani et al. scale embedding weights by sqrt(d)\n",
        "    return unscaled * (self.d ** .5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mW1B_QZKtSdM"
      },
      "outputs": [],
      "source": [
        "# note that these are the numbers\n",
        "# max_q_seq_length = 160\n",
        "# max_a_seq_length = 30\n",
        "class PositionalEncoding(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Performs a positional encoding on a sequence of tokens.\n",
        "  Vaswani et al. section 3.5\n",
        "  \"\"\"\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    :param x: (B, n, d) tensor of B sets of n embedded tokens\n",
        "    :return: (B, n, d) tensor of B sets of n embedded tokens with\n",
        "    positional encoding\n",
        "    \"\"\"\n",
        "    # make a matrix that is the sum of the input and the matrix of\n",
        "    # sin and cos PE from 3.5\n",
        "\n",
        "    # need to update pos\n",
        "\n",
        "    pe = torch.zeros(x.size())\n",
        "    b, n, d_model = x.size()\n",
        "\n",
        "    # require d_model even to make our lives easy\n",
        "    assert d_model % 2 == 0\n",
        "\n",
        "    pos = torch.arange(n)\n",
        "    i = torch.arange(d_model // 2)\n",
        "    large_num = 10000 ** (2 * i / d_model)\n",
        "    angle = pos[:, None] / large_num[None, :]\n",
        "    pe[:, :, ::2] = torch.sin(angle)\n",
        "    pe[:, :, 1::2] = torch.cos(angle)\n",
        "\n",
        "    return x + pe.to(x.device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "F2_iC4m7tk_3"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNetwork(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  A 2-layer feed-forward neural network.\n",
        "  Vaswani et al. section 3.3\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, d_model, d_ff):\n",
        "    \"\"\"\n",
        "    :param d_model: dimension of input and output token embeddings\n",
        "    :param d_ff: dimension of hidden layer\n",
        "    \"\"\"\n",
        "    # use these dimensions for the architecture of FFN\n",
        "    # (see 3.3 for details)\n",
        "    self.d_model = d_model\n",
        "    self.d_ff = d_ff\n",
        "\n",
        "    super().__init__()\n",
        "    self.hidden = torch.nn.Linear(d_model, d_ff)\n",
        "    self.hidden2 = torch.nn.Linear(d_ff, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    :param x: (B, n, d) tensor B sets of n of embedded tokens\n",
        "    :return: (B, n, d) tensor B sets of n of tokens transformed by an MLP\n",
        "    \"\"\"\n",
        "    x = torch.nn.functional.relu(self.hidden(x))\n",
        "    x = self.hidden2(x)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ji_oi9e8tQ3o"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Scaled dot-product attention.\n",
        "  Vaswani et al. section 3.2.1\n",
        "  \"\"\"\n",
        "\n",
        "  def forward(self, queries, keys, values, mask=None):\n",
        "    \"\"\"\n",
        "    :param queries: (B, n, d_a) tensor B sets of query token embeddings\n",
        "    :param keys: (B, n, d_a) tensor B sets key token embeddings\n",
        "    :param values: (B, n, d_a) tensor B sets of value embeddings\n",
        "    :param mask: (B, n, n) tensor B sets of matrices of either 0 or -inf\n",
        "    to mask out certain key/query relationships. None indicates a mask\n",
        "    of all 0s (no masking)\n",
        "    :return: (B, n, d_a) tensor of B sets of scaled dot-product\n",
        "    attention of queries, keys and values with masking given by mask\n",
        "    \"\"\"\n",
        "    if (mask == None):\n",
        "      return torch.matmul(torch.nn.functional.softmax(torch.matmul(queries,torch.transpose(keys,1,2))/(keys.size()[2] ** .5),dim=2),values)\n",
        "    else:\n",
        "      return torch.matmul(torch.nn.functional.softmax(torch.add(mask,torch.matmul(queries,torch.transpose(keys,1,2))/(keys.size()[2] ** .5)),dim=2),values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Euky98jZtPAg"
      },
      "outputs": [],
      "source": [
        "class MultiheadAttention(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Multi-head attention.\n",
        "  Vaswani et al. section 3.2.2\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, h: int, d: int):\n",
        "    \"\"\"\n",
        "    :param h: number of attention heads to use\n",
        "    :param d: dimension of incoming token embeddings. Must have h | d\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    assert d % h == 0  # <=> h | d\n",
        "\n",
        "    self.h = h\n",
        "    self.d_head = d // h\n",
        "    self.attention = ScaledDotProductAttention()\n",
        "\n",
        "    self.query_projection = torch.nn.Linear(d, d, bias=False)\n",
        "    self.key_projection = torch.nn.Linear(d, d, bias=False)\n",
        "    self.value_projection = torch.nn.Linear(d, d, bias=False)\n",
        "    self.concat_projection = torch.nn.Linear(d, d, bias=False)\n",
        "\n",
        "  def forward(self, queries, keys, values, mask=None):\n",
        "    \"\"\"\n",
        "    :param queries: (B, n_q, d) tensor B sets of query token embeddings\n",
        "    :param keys: (B, n_k, d) tensor B sets key token embeddings\n",
        "    :param values: (B, n_k, d) tensor B sets of value embeddings\n",
        "    :param mask: (B, n_q, n_k) tensor B sets of matrices of either 0 or\n",
        "    -inf to mask out certain key/query relationships. None indicates\n",
        "    a mask of all 0s (no masking)\n",
        "    :return: (B, n, d) tensor of B sets of multihead scaled\n",
        "    dot-product attention of queries, keys and values with masking\n",
        "    given by mask\n",
        "    \"\"\"\n",
        "    # shapes of head inputs (B, n_i, h, d_head), i=q,k,v\n",
        "    q_input_shape = queries.shape[:2] + (self.h, self.d_head)\n",
        "    k_input_shape = keys.shape[:2] + (self.h, self.d_head)\n",
        "    v_input_shape = keys.shape[:2] + (self.h, self.d_head)\n",
        "\n",
        "    # get projections and split along embedding dimension\n",
        "    queries = self.query_projection(queries).view(q_input_shape)\n",
        "    keys = self.key_projection(keys).view(k_input_shape)\n",
        "    values = self.value_projection(values).view(v_input_shape)\n",
        "\n",
        "    # perform attention on each head\n",
        "    mha = torch.cat([\n",
        "      self.attention(\n",
        "          queries[:, :, i, :], keys[:, :, i, :], values[:, :, i, :], mask\n",
        "      )\n",
        "      for i in range(self.h)\n",
        "    ], dim=2)\n",
        "\n",
        "    # output projection\n",
        "    return self.concat_projection(mha)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1UG2R5Q-tHLS"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Transformer encoder layer.\n",
        "  Vaswani et al. section 3.1\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self, ffn: FeedForwardNetwork, n_heads: int, p_dropout: float = 0.1\n",
        "  ):\n",
        "    \"\"\"\n",
        "    :param ffn: the feed-forward network\n",
        "    :param n_heads: the number of attention heads to use in multihead\n",
        "    attention\n",
        "    :param p_dropout: dropout probability\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.dropout = torch.nn.Dropout(p_dropout)\n",
        "\n",
        "    self.attention = MultiheadAttention(n_heads, ffn.d_model)\n",
        "    self.layer_norm1 = torch.nn.LayerNorm(ffn.d_model)\n",
        "\n",
        "    self.ffn = ffn\n",
        "    self.layer_norm2 = torch.nn.LayerNorm(ffn.d_model)\n",
        "\n",
        "  def forward(self, x, mask_in=None):\n",
        "    \"\"\"\n",
        "    :param x: (B, n, d) tensor B sets of n token embeddings\n",
        "    :param mask_in: (B, n, n) tensor of B matrices to mask out padding\n",
        "    tokens\n",
        "    :return: (B, n, d) tensor B sets of n token embeddings after one\n",
        "    Transformer Encoder layer\n",
        "    \"\"\"\n",
        "    y = self.dropout(self.attention(x, x, x, mask_in))\n",
        "    x = self.layer_norm1(x + y)\n",
        "\n",
        "    y = self.dropout(self.ffn(x))\n",
        "    x = self.layer_norm2(x + y)\n",
        "\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "d22qGTgVtDSK"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Transformer decoder layer.\n",
        "  Vaswani et al. section 3.1\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self, ffn: FeedForwardNetwork, n_heads: int, p_dropout: float = 0.1\n",
        "  ):\n",
        "    \"\"\"\n",
        "    :param ffn: the feed-forward network to use in the decoder\n",
        "    :param n_heads: the number of attention heads to use in multihead\n",
        "    attention\n",
        "    :param p_dropout: dropout probability\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.dropout = torch.nn.Dropout(p_dropout)\n",
        "\n",
        "    self.attention = MultiheadAttention(n_heads, ffn.d_model)\n",
        "    self.layer_norm1 = torch.nn.LayerNorm(ffn.d_model)\n",
        "\n",
        "    self.cross_attention = MultiheadAttention(n_heads, ffn.d_model)\n",
        "    self.layer_norm2 = torch.nn.LayerNorm(ffn.d_model)\n",
        "\n",
        "    self.ffn = ffn\n",
        "    self.layer_norm3 = torch.nn.LayerNorm(ffn.d_model)\n",
        "\n",
        "  def forward(self, x_in, x_out, output_mask=None, cross_mask=None):\n",
        "    \"\"\"\n",
        "    :param x_in: (B, n_in, d) tensor B sets of n_in input token\n",
        "    embeddings\n",
        "    :param x_out: (B, n_out, d) tensor B sets of n_out output token\n",
        "    embeddings\n",
        "    :param output_mask: (B, n_out, n_out) tensor B sets of matrices to\n",
        "    mask out certain relationships between output tokens (to prevent\n",
        "    backward flow of information during training). A value of None\n",
        "    indicates no masking\n",
        "    :param cross_mask: (B, n_out, n_in) tensor B sets of matrices to mask\n",
        "    out input padding tokens in cross attention\n",
        "    :return: (B, n_out, d) tensor B sets of n_out output embeddings after\n",
        "    one Transformer decoder layer\n",
        "    \"\"\"\n",
        "    x = x_out\n",
        "\n",
        "    y = self.dropout(self.attention(x, x, x, output_mask))\n",
        "    x = self.layer_norm1(x + y)\n",
        "\n",
        "    y = self.dropout(self.cross_attention(x, x_in, x_in, cross_mask))\n",
        "    x = self.layer_norm2(x + y)\n",
        "\n",
        "    y = self.dropout(self.ffn(x))\n",
        "    x = self.layer_norm3(x + y)\n",
        "\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "bluGL7K1wDGs"
      },
      "outputs": [],
      "source": [
        "class Transformer(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Complete implementation of the transformer model in Vaswani et al.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      token_embedding: TokenEmbedding,\n",
        "      max_output_length: int,\n",
        "      n_encoder_layers: int,\n",
        "      n_decoder_layers: int,\n",
        "      n_heads: int,\n",
        "      d_ff: int,\n",
        "      p_dropout: float = 0.1\n",
        "  ):\n",
        "    \"\"\"\n",
        "    :param token_embedding: The TokenEmbedding object to use for token\n",
        "    embedding\n",
        "    :param max_output_length: The maximum sequence length (excluding\n",
        "    <START> and <END> tokens) to generate during inference.\n",
        "    :param n_encoder_layers: How many encoder layers to use\n",
        "    :param n_decoder_layers: How many decoder layers to use\n",
        "    :param n_heads: number of attention heads to use in multihead\n",
        "    attention\n",
        "    :param d_ff: dimension of feed-forward networks in encoder/decoder\n",
        "    layers\n",
        "    :param p_dropout: Dropout probability\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.max_output_length = max_output_length\n",
        "    self.token_embedding = token_embedding\n",
        "    self.positional_encoding = PositionalEncoding()\n",
        "    self.dropout = torch.nn.Dropout(p_dropout)\n",
        "\n",
        "    self.encoder_layers = torch.nn.ModuleList([\n",
        "        EncoderLayer(\n",
        "            FeedForwardNetwork(self.token_embedding.d, d_ff),\n",
        "            n_heads, p_dropout\n",
        "        )\n",
        "        for _ in range(n_encoder_layers)\n",
        "    ])\n",
        "\n",
        "    self.decoder_layers = torch.nn.ModuleList([\n",
        "        DecoderLayer(\n",
        "            FeedForwardNetwork(self.token_embedding.d, d_ff),\n",
        "            n_heads, p_dropout\n",
        "        )\n",
        "        for _ in range(n_decoder_layers)\n",
        "    ])\n",
        "\n",
        "  def input_attention_mask(self, input_indices):\n",
        "    \"\"\"\n",
        "    :param input_indices: (B, n) tensor of B sets of input indices\n",
        "    to make an input attention mask for\n",
        "    :return: (B, n, n) mask that ignores padding tokens in input\n",
        "    \"\"\"\n",
        "    if self.training:\n",
        "      # only need to mask *columns* involving padding tokens in the\n",
        "      # attention matrix. Values generated from the unmasked rows will\n",
        "      # be masked in subsequent layers (and in cross-atttention layers).\n",
        "      b, n = input_indices.shape\n",
        "      col_mask = torch.where(\n",
        "          input_indices == self.token_embedding.pad_index,\n",
        "          -float('inf'), 0.\n",
        "      ).view(b, 1, n)\n",
        "\n",
        "      return torch.broadcast_to(col_mask, (b, n, n))\n",
        "\n",
        "  def cross_attention_mask(self, input_indices, output_indices):\n",
        "    \"\"\"\n",
        "    :param input_indices: (B, n_in) tensor of B sets of input token\n",
        "    indices\n",
        "    :param output_indices: (B, n_out) tensor of B sets of output token\n",
        "    indices\n",
        "    :return: (B, n_out, n_in) mask that ignores padding tokens in input\n",
        "    \"\"\"\n",
        "    # in cross attention, the inputs are used as keys and values, so\n",
        "    # masking out columns in the attention matrix will deal with padding\n",
        "    # tokens in both\n",
        "    # keys and values\n",
        "    (b, n_in), n_out = input_indices.shape, output_indices.shape[1]\n",
        "    col_mask = torch.where(\n",
        "        input_indices == self.token_embedding.pad_index,\n",
        "        -float('inf'), 0.\n",
        "    ).view(b, 1, n_in)\n",
        "\n",
        "    return torch.broadcast_to(col_mask, (b, n_out, n_in))\n",
        "\n",
        "  def output_attention_mask(self, output_indices):\n",
        "    \"\"\"\n",
        "    :param output_indices: (B, n) tensor of B sets of output indices\n",
        "    to make an output self-attention mask for\n",
        "    :return: (B, n, n) attention mask that masks out attention between\n",
        "    tokens and subsequent tokens as well as padding tokens\n",
        "    \"\"\"\n",
        "    # for output attention, we need to mask *columns* to the right\n",
        "    # of the diagonal as well as *columns* that correspond to\n",
        "    # padding tokens\n",
        "    b, n = output_indices.shape\n",
        "    col_mask = torch.where(\n",
        "        output_indices == self.token_embedding.pad_index,\n",
        "        -float('inf'), 0.\n",
        "    ).view(b, 1, n)\n",
        "    pad_mask = torch.broadcast_to(col_mask, (b, n, n))\n",
        "\n",
        "    # ignoring future output tokens results in an upper-triangular mask\n",
        "    flow_mask = torch.triu(\n",
        "        -float('inf') * torch.ones_like(pad_mask), diagonal=1\n",
        "    )\n",
        "\n",
        "    # logical OR <-> addition when False <-> 0 and True <-> -inf\n",
        "    return pad_mask + flow_mask\n",
        "\n",
        "  def next_token(\n",
        "      self, input_encoding, output_pe, output_mask=None, cross_mask=None\n",
        "  ):\n",
        "    \"\"\"\n",
        "    :param input_encoding: (B, n_in, d) input token encoding\n",
        "    :param output_pe: (B, n_out, d) output token embedding with\n",
        "    positional encoding\n",
        "    :output_mask: (B, n_out, n_out) mask for output attention\n",
        "    :cross_mask: (B, n_out, n_in) mask for cross attention\n",
        "    :return: (B, num_tokens, n_out) tensor B sets of next-token\n",
        "    scores (logits) for each output token\n",
        "    \"\"\"\n",
        "    b, n_out = output_pe.shape[:2]\n",
        "\n",
        "    x = output_pe\n",
        "    for decoder in self.decoder_layers:\n",
        "      x = decoder(input_encoding, x, output_mask, cross_mask)\n",
        "\n",
        "    # use embedding matrix as projection to token scores\n",
        "    scores = torch.einsum('ijk,lk', x, self.token_embedding.embedding_matrix)\n",
        "\n",
        "    # return logits, as PyTorch will compute the softmax in\n",
        "    # CrossEntropyLoss for us. Note that CrossEntropyLoss requires num_tokens\n",
        "    # dimension to be the second dimension, so we need to transpose\n",
        "    return torch.transpose(scores, 1, 2)\n",
        "\n",
        "  def forward(self, x: List[str] | str, y: List[str] | None=None):\n",
        "    \"\"\"\n",
        "    :param x: list of B strings of input tokens during training. A single\n",
        "    string during inference.\n",
        "    :param y: list of B string of ground-truth output tokens. If provided,\n",
        "    the model uses the ground-truth output tokens to compute next-token\n",
        "    probabilities (masking out future output tokens). This is used\n",
        "    for training and also to evaluate accuracy during testing.\n",
        "    :return: Depends on whether y is given or not.\n",
        "    if y is not None:\n",
        "      (prob, actual)\n",
        "        prob: (B, n_tokens, n_out - 1) B matrices of predicted next-token\n",
        "        logit scores; prob[b, j, i] = log of probability that output token\n",
        "        i is token j given first i output tokens within question-answer\n",
        "        pair b\n",
        "        actual: (B, n_out - 1) tensor B sets of actual next-token indices\n",
        "\n",
        "    if y is None:\n",
        "      prediction: Predicted output string\n",
        "    \"\"\"\n",
        "    if not self.training:\n",
        "      x = [x]\n",
        "\n",
        "    input_indices = self.token_embedding.indices(x)\n",
        "    input_embedding = self.token_embedding(input_indices)\n",
        "    input_pe = self.dropout(self.positional_encoding(input_embedding))\n",
        "    input_mask = self.input_attention_mask(input_indices)\n",
        "\n",
        "    input_encoding = input_pe\n",
        "    for encoder_layer in self.encoder_layers:\n",
        "       input_encoding = encoder_layer(input_encoding, input_mask)\n",
        "\n",
        "    if y is not None:\n",
        "      output_indices = self.token_embedding.indices(y)\n",
        "      output_embedding = self.token_embedding(output_indices)\n",
        "      output_pe = self.dropout(self.positional_encoding(output_embedding))\n",
        "      output_mask = self.output_attention_mask(output_indices)\n",
        "      cross_mask = self.cross_attention_mask(input_indices, output_indices)\n",
        "\n",
        "      # just do one forward pass to get next-token log-probabilities\n",
        "      prob = self.next_token(input_encoding, output_pe, output_mask, cross_mask)\n",
        "\n",
        "      # actual is just the output indices shifted left 1\n",
        "      # make sure to drop the last probability output\n",
        "      return prob[..., :-1], output_indices[:, 1:]\n",
        "\n",
        "    else:\n",
        "      # initialize output with a <start> token\n",
        "      output_indices = torch.tensor(\n",
        "          [[self.token_embedding.start_index]],\n",
        "          dtype=torch.long, device=input_encoding.device\n",
        "      )\n",
        "\n",
        "      # autoregressively compute next tokens\n",
        "      while True:\n",
        "        output_embedding = self.token_embedding(output_indices)\n",
        "        output_pe = self.positional_encoding(output_embedding)\n",
        "\n",
        "        # get next-token scores, take only last prediction (we don't\n",
        "        # care about scores for already-generated tokens)\n",
        "        prob = self.next_token(input_encoding, output_pe)[0, :, -1]\n",
        "        print(prob)\n",
        "\n",
        "        # append most likely token to current output\n",
        "        best_token_index = prob.argmax()\n",
        "        next_token = torch.tensor(\n",
        "            [[best_token_index]],\n",
        "            dtype=torch.long, device=input_encoding.device\n",
        "        )\n",
        "        output_indices = torch.cat([output_indices, next_token], dim=1)\n",
        "\n",
        "        # return if we reach the maximum output length or if the model emits\n",
        "        # an <EOS> token\n",
        "        if best_token_index == self.token_embedding.end_index:\n",
        "          final_indices = output_indices.cpu()\n",
        "          break\n",
        "\n",
        "        elif len(output_indices) == self.max_output_length + 1:\n",
        "          # if we hit max length, then make sure to add an <EOS> token\n",
        "          final_indices = torch.cat([\n",
        "              output_indices.cpu(),\n",
        "              torch.tensor([[self.token_embedding.end_index]])\n",
        "          ], dim=1)\n",
        "          break\n",
        "\n",
        "      # convert token indices back to string\n",
        "      return self.token_embedding.unembed(final_indices)[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7hziDg2xdn7"
      },
      "source": [
        "## Setup for Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MC016ECxfuA"
      },
      "source": [
        "Determine the token set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "19tVL2AgwhFI"
      },
      "outputs": [],
      "source": [
        "def token_set(math_dataset, must_include=()):\n",
        "  seen_tokens = set(must_include)\n",
        "  for question, answer in math_dataset:\n",
        "    # Just use lower-case characters as tokens\n",
        "    seen_tokens.update(question.lower())\n",
        "    seen_tokens.update(answer.lower())\n",
        "\n",
        "  return tuple(sorted(seen_tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd0ZiueUx31A"
      },
      "source": [
        "Determine token set for arithmetic training easy. Takes ~30s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAAncYCLx3oG",
        "outputId": "9e43e835-5829-461e-bc5b-eff0f4ace74d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(' ',\n",
              " '(',\n",
              " ')',\n",
              " '*',\n",
              " '+',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '/',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " '?',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load all arithmetic training easy subcategories\n",
        "arthimetic_easy_subcats = MathDataset.subcategories()['train-easy']['arithmetic']\n",
        "datasets = [\n",
        "    MathDataset('train-easy', 'arithmetic', s)\n",
        "    for s in arthimetic_easy_subcats\n",
        "]\n",
        "\n",
        "# put together all subcategories\n",
        "arithmetic_easy = torch.utils.data.ConcatDataset(datasets)\n",
        "\n",
        "# find all possible tokens\n",
        "arithmetic_easy_tokens = token_set(arithmetic_easy)\n",
        "\n",
        "arithmetic_easy_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwq9wxIB5lvA"
      },
      "source": [
        "Evaluate input-output sequence lengths. Takes ~1min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "id": "tfmprD4E5lBI",
        "outputId": "b69f5521-e092-422d-d11b-e8fa1ffb4f88"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "q_lengths, a_lengths = [], []\n",
        "for q, a in arithmetic_easy:\n",
        "  q_lengths.append(len(q))\n",
        "  a_lengths.append(len(a))\n",
        "\n",
        "\n",
        "plt.hist(q_lengths, density=True)\n",
        "plt.show()\n",
        "plt.hist(a_lengths, density=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llb32_RELpMa"
      },
      "source": [
        "## Exploring Token Embeddings and Attention Masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "VfiUh707Lp_6"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(\n",
        "    TokenEmbedding(arithmetic_easy_tokens, 6),  # 6 = d_model\n",
        "    30,   # = max_output_length\n",
        "    5,    # = n_encoder_layers\n",
        "    5,    # = n_decoder_layers\n",
        "    1,    # = n_heads\n",
        "    1024  # = d_ff\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KRegsZ1Pf3v",
        "outputId": "f1b89bbd-713e-4fd5-dc12-01fda21e38eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['What is -5 - 110911?',\n",
              " 'What is -0.188 + -0.814?',\n",
              " 'Sum 259 and -46.',\n",
              " 'Sum -10 and -52539.',\n",
              " 'What is the difference between -2 and 251860?']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_questions = [arithmetic_easy[i][0] for i in range(5)]\n",
        "sample_questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixuSqIznOtU-",
        "outputId": "8f5ca9a3-6f94-4241-be7e-fd0a5b673383"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0, 44, 30, 23, 41,  3, 31, 40,  3,  9, 17,  3,  9,  3, 13, 13, 12, 21,\n",
              "         13, 13, 22,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
              "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
              "        [ 0, 44, 30, 23, 41,  3, 31, 40,  3,  9, 12, 10, 13, 20, 20,  3,  7,  3,\n",
              "          9, 12, 10, 20, 13, 16, 22,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
              "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
              "        [ 0, 40, 42, 34,  3, 14, 17, 21,  3, 23, 35, 26,  3,  9, 16, 18, 10,  1,\n",
              "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
              "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
              "        [ 0, 40, 42, 34,  3,  9, 13, 12,  3, 23, 35, 26,  3,  9, 17, 14, 17, 15,\n",
              "         21, 10,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
              "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
              "        [ 0, 44, 30, 23, 41,  3, 31, 40,  3, 41, 30, 27,  3, 26, 31, 28, 28, 27,\n",
              "         39, 27, 35, 25, 27,  3, 24, 27, 41, 44, 27, 27, 35,  3,  9, 14,  3, 23,\n",
              "         35, 26,  3, 14, 17, 13, 20, 18, 12, 22,  1]])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_indices = transformer.token_embedding.indices(sample_questions)\n",
        "sample_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZKdL-1kTGry",
        "outputId": "800129f0-8bf9-45f1-906c-a6f73b135715"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['what is -5 - 110911?',\n",
              " 'what is -0.188 + -0.814?',\n",
              " 'sum 259 and -46.',\n",
              " 'sum -10 and -52539.',\n",
              " 'what is the difference between -2 and 251860?']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_unembedded = transformer.token_embedding.unembed(sample_indices)\n",
        "sample_unembedded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YBz97qNTOES",
        "outputId": "ab44af5c-df47-466f-dea6-6f2e5a6eae4a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<START>what is -5 - 110911?<END><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>',\n",
              " '<START>what is -0.188 + -0.814?<END><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>',\n",
              " '<START>sum 259 and -46.<END><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>',\n",
              " '<START>sum -10 and -52539.<END><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>',\n",
              " '<START>what is the difference between -2 and 251860?<END>']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_unembedded_special = transformer.token_embedding.unembed(\n",
        "    sample_indices, include_special=True\n",
        ")\n",
        "sample_unembedded_special"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oswg-yHLQaL9",
        "outputId": "4043d7e4-5bf7-459d-8d52-d82ea798a3a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[-5.6530,  2.9861,  0.0335, -1.2978, -3.2770, -3.3343],\n",
              "         [ 2.4446,  5.3501, -5.4558, -1.4338,  1.4089, -0.3575],\n",
              "         [ 3.9087,  0.5899,  0.2817, -0.8572,  5.7560, -0.5077],\n",
              "         ...,\n",
              "         [-0.2730,  2.0286, -0.2353,  1.7067,  3.6919,  0.1826],\n",
              "         [-0.2730,  2.0286, -0.2353,  1.7067,  3.6919,  0.1826],\n",
              "         [-0.2730,  2.0286, -0.2353,  1.7067,  3.6919,  0.1826]],\n",
              "\n",
              "        [[-5.6530,  2.9861,  0.0335, -1.2978, -3.2770, -3.3343],\n",
              "         [ 2.4446,  5.3501, -5.4558, -1.4338,  1.4089, -0.3575],\n",
              "         [ 3.9087,  0.5899,  0.2817, -0.8572,  5.7560, -0.5077],\n",
              "         ...,\n",
              "         [-0.2730,  2.0286, -0.2353,  1.7067,  3.6919,  0.1826],\n",
              "         [-0.2730,  2.0286, -0.2353,  1.7067,  3.6919,  0.1826],\n",
              "         [-0.2730,  2.0286, -0.2353,  1.7067,  3.6919,  0.1826]],\n",
              "\n",
              "        [[-5.6530,  2.9861,  0.0335, -1.2978, -3.2770, -3.3343],\n",
              "         [-0.1036,  1.5104, -6.8446,  4.8069, -0.1835, -1.4648],\n",
              "         [-4.8159, -1.4185, -2.8177,  1.9061, -8.7705,  0.6051],\n",
              "         ...,\n",
              "         [-0.2730,  2.0286, -0.2353,  1.7067,  3.6919,  0.1826],\n",
              "         [-0.2730,  2.0286, -0.2353,  1.7067,  3.6919,  0.1826],\n",
              "         [-0.2730,  2.0286, -0.2353,  1.7067,  3.6919,  0.1826]],\n",
              "\n",
              "        [[-5.6530,  2.9861,  0.0335, -1.2978, -3.2770, -3.3343],\n",
              "         [-0.1036,  1.5104, -6.8446,  4.8069, -0.1835, -1.4648],\n",
              "         [-4.8159, -1.4185, -2.8177,  1.9061, -8.7705,  0.6051],\n",
              "         ...,\n",
              "         [-0.2730,  2.0286, -0.2353,  1.7067,  3.6919,  0.1826],\n",
              "         [-0.2730,  2.0286, -0.2353,  1.7067,  3.6919,  0.1826],\n",
              "         [-0.2730,  2.0286, -0.2353,  1.7067,  3.6919,  0.1826]],\n",
              "\n",
              "        [[-5.6530,  2.9861,  0.0335, -1.2978, -3.2770, -3.3343],\n",
              "         [ 2.4446,  5.3501, -5.4558, -1.4338,  1.4089, -0.3575],\n",
              "         [ 3.9087,  0.5899,  0.2817, -0.8572,  5.7560, -0.5077],\n",
              "         ...,\n",
              "         [-0.9324, -0.4695, -3.4444,  0.7985, -6.1896,  2.5729],\n",
              "         [ 1.2304, -3.1492,  1.0027, -0.7977, -0.2463, -1.8550],\n",
              "         [ 1.7001, -4.1902,  2.2583,  1.2765, -7.6649, -0.0580]]],\n",
              "       grad_fn=<MulBackward0>)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_token_embeddings = transformer.token_embedding(sample_indices)\n",
        "sample_token_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2_7C5k4RfCk",
        "outputId": "17737571-b840-4ded-9cbc-11c210302be0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token sequence indices and input mask matrix\n",
            "0 44 30 23 41 3 31 40 3 9 17 3 9 3 13 13 12 21 13 13 22 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "\n",
            "Post-softmax\n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "\n",
            "Token sequence indices and input mask matrix\n",
            "0 44 30 23 41 3 31 40 3 9 12 10 13 20 20 3 7 3 9 12 10 20 13 16 22 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "\n",
            "Post-softmax\n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "\n",
            "Token sequence indices and input mask matrix\n",
            "0 40 42 34 3 14 17 21 3 23 35 26 3 9 16 18 10 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "\n",
            "Post-softmax\n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "\n",
            "Token sequence indices and input mask matrix\n",
            "0 40 42 34 3 9 13 12 3 23 35 26 3 9 17 14 17 15 21 10 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "\n",
            "Post-softmax\n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "\n",
            "Token sequence indices and input mask matrix\n",
            "0 44 30 23 41 3 31 40 3 41 30 27 3 26 31 28 28 27 39 27 35 25 27 3 24 27 41 44 27 27 35 3 9 14 3 23 35 26 3 14 17 13 20 18 12 22 1 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "\n",
            "Post-softmax\n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "def print_full(matrix, digits='.02f'):\n",
        "  for r in range(matrix.shape[0]):\n",
        "    for c in range(matrix.shape[1]):\n",
        "      print(f'{float(matrix[r, c]):{digits}}', end=' ')\n",
        "    print()\n",
        "  print()\n",
        "\n",
        "\n",
        "mask = transformer.input_attention_mask(sample_indices)\n",
        "for index_seq, mask in zip(sample_indices, mask):\n",
        "  print('Token sequence indices and input mask matrix')\n",
        "  print_full(torch.cat([index_seq[None], mask]), digits='g')\n",
        "\n",
        "  print('Post-softmax')\n",
        "  print_full(torch.nn.functional.softmax(mask, dim=1), digits='.4f')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "6Mi4DIcgSaHG"
      },
      "outputs": [],
      "source": [
        "sample_outputs = [arithmetic_easy[i][1] for i in range(5)]\n",
        "sample_output_indices = transformer.token_embedding.indices(sample_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmESJ0IYW7oO",
        "outputId": "ee4c89e6-b482-426a-a3d0-fe9311e55ed3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['-110916', '-1.002', '213', '-52549', '251862']"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fksug2sLW9IJ",
        "outputId": "a5b05068-cf62-4667-a4d4-3eb3b63d319b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0,  9, 13, 13, 12, 21, 13, 18,  1],\n",
              "        [ 0,  9, 13, 10, 12, 12, 14,  1,  2],\n",
              "        [ 0, 14, 13, 15,  1,  2,  2,  2,  2],\n",
              "        [ 0,  9, 17, 14, 17, 16, 21,  1,  2],\n",
              "        [ 0, 14, 17, 13, 20, 18, 14,  1,  2]])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_output_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ubq0_CoW-tj",
        "outputId": "acacb810-b2b9-4ade-ac96-78cd5d23982f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token sequence indices and output self-attention mask matrix\n",
            "0 9 13 13 12 21 13 18 1 \n",
            "0 -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 -inf -inf -inf \n",
            "0 0 0 0 0 0 0 -inf -inf \n",
            "0 0 0 0 0 0 0 0 -inf \n",
            "0 0 0 0 0 0 0 0 0 \n",
            "\n",
            "Post-softmax\n",
            "1.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.5000 0.5000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.3333 0.3333 0.3333 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.2500 0.2500 0.2500 0.2500 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.2000 0.2000 0.2000 0.2000 0.2000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.1667 0.1667 0.1667 0.1667 0.1667 0.1667 0.0000 0.0000 0.0000 \n",
            "0.1429 0.1429 0.1429 0.1429 0.1429 0.1429 0.1429 0.0000 0.0000 \n",
            "0.1250 0.1250 0.1250 0.1250 0.1250 0.1250 0.1250 0.1250 0.0000 \n",
            "0.1111 0.1111 0.1111 0.1111 0.1111 0.1111 0.1111 0.1111 0.1111 \n",
            "\n",
            "Token sequence indices and output self-attention mask matrix\n",
            "0 9 13 10 12 12 14 1 2 \n",
            "0 -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 -inf -inf -inf \n",
            "0 0 0 0 0 0 0 -inf -inf \n",
            "0 0 0 0 0 0 0 0 -inf \n",
            "0 0 0 0 0 0 0 0 -inf \n",
            "\n",
            "Post-softmax\n",
            "1.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.5000 0.5000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.3333 0.3333 0.3333 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.2500 0.2500 0.2500 0.2500 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.2000 0.2000 0.2000 0.2000 0.2000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.1667 0.1667 0.1667 0.1667 0.1667 0.1667 0.0000 0.0000 0.0000 \n",
            "0.1429 0.1429 0.1429 0.1429 0.1429 0.1429 0.1429 0.0000 0.0000 \n",
            "0.1250 0.1250 0.1250 0.1250 0.1250 0.1250 0.1250 0.1250 0.0000 \n",
            "0.1250 0.1250 0.1250 0.1250 0.1250 0.1250 0.1250 0.1250 0.0000 \n",
            "\n",
            "Token sequence indices and output self-attention mask matrix\n",
            "0 14 13 15 1 2 2 2 2 \n",
            "0 -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 -inf -inf -inf -inf \n",
            "0 0 0 0 0 -inf -inf -inf -inf \n",
            "0 0 0 0 0 -inf -inf -inf -inf \n",
            "0 0 0 0 0 -inf -inf -inf -inf \n",
            "0 0 0 0 0 -inf -inf -inf -inf \n",
            "\n",
            "Post-softmax\n",
            "1.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.5000 0.5000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.3333 0.3333 0.3333 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.2500 0.2500 0.2500 0.2500 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.2000 0.2000 0.2000 0.2000 0.2000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.2000 0.2000 0.2000 0.2000 0.2000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.2000 0.2000 0.2000 0.2000 0.2000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.2000 0.2000 0.2000 0.2000 0.2000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.2000 0.2000 0.2000 0.2000 0.2000 0.0000 0.0000 0.0000 0.0000 \n",
            "\n",
            "Token sequence indices and output self-attention mask matrix\n",
            "0 9 17 14 17 16 21 1 2 \n",
            "0 -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 -inf -inf -inf \n",
            "0 0 0 0 0 0 0 -inf -inf \n",
            "0 0 0 0 0 0 0 0 -inf \n",
            "0 0 0 0 0 0 0 0 -inf \n",
            "\n",
            "Post-softmax\n",
            "1.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.5000 0.5000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.3333 0.3333 0.3333 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.2500 0.2500 0.2500 0.2500 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.2000 0.2000 0.2000 0.2000 0.2000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.1667 0.1667 0.1667 0.1667 0.1667 0.1667 0.0000 0.0000 0.0000 \n",
            "0.1429 0.1429 0.1429 0.1429 0.1429 0.1429 0.1429 0.0000 0.0000 \n",
            "0.1250 0.1250 0.1250 0.1250 0.1250 0.1250 0.1250 0.1250 0.0000 \n",
            "0.1250 0.1250 0.1250 0.1250 0.1250 0.1250 0.1250 0.1250 0.0000 \n",
            "\n",
            "Token sequence indices and output self-attention mask matrix\n",
            "0 14 17 13 20 18 14 1 2 \n",
            "0 -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 -inf -inf -inf \n",
            "0 0 0 0 0 0 0 -inf -inf \n",
            "0 0 0 0 0 0 0 0 -inf \n",
            "0 0 0 0 0 0 0 0 -inf \n",
            "\n",
            "Post-softmax\n",
            "1.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.5000 0.5000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.3333 0.3333 0.3333 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.2500 0.2500 0.2500 0.2500 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.2000 0.2000 0.2000 0.2000 0.2000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.1667 0.1667 0.1667 0.1667 0.1667 0.1667 0.0000 0.0000 0.0000 \n",
            "0.1429 0.1429 0.1429 0.1429 0.1429 0.1429 0.1429 0.0000 0.0000 \n",
            "0.1250 0.1250 0.1250 0.1250 0.1250 0.1250 0.1250 0.1250 0.0000 \n",
            "0.1250 0.1250 0.1250 0.1250 0.1250 0.1250 0.1250 0.1250 0.0000 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "mask = transformer.output_attention_mask(sample_output_indices)\n",
        "for index_seq, mask in zip(sample_output_indices, mask):\n",
        "  print('Token sequence indices and output self-attention mask matrix')\n",
        "  print_full(torch.cat([index_seq[None], mask]), digits='g')\n",
        "\n",
        "  print('Post-softmax')\n",
        "  print_full(torch.nn.functional.softmax(mask, dim=1), digits='.4f')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdSlMn0WXJdG",
        "outputId": "df78f36c-9515-4fb9-e91e-67defb3b9a63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token sequence indices and output cross-attention mask matrix\n",
            "0 44 30 23 41 3 31 40 3 9 17 3 9 3 13 13 12 21 13 13 22 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "\n",
            "Post-softmax\n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0455 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "\n",
            "Token sequence indices and output cross-attention mask matrix\n",
            "0 44 30 23 41 3 31 40 3 9 12 10 13 20 20 3 7 3 9 12 10 20 13 16 22 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "\n",
            "Post-softmax\n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0385 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "\n",
            "Token sequence indices and output cross-attention mask matrix\n",
            "0 40 42 34 3 14 17 21 3 23 35 26 3 9 16 18 10 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "\n",
            "Post-softmax\n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0556 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "\n",
            "Token sequence indices and output cross-attention mask matrix\n",
            "0 40 42 34 3 9 13 12 3 23 35 26 3 9 17 14 17 15 21 10 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf \n",
            "\n",
            "Post-softmax\n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0476 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
            "\n",
            "Token sequence indices and output cross-attention mask matrix\n",
            "0 44 30 23 41 3 31 40 3 41 30 27 3 26 31 28 28 27 39 27 35 25 27 3 24 27 41 44 27 27 35 3 9 14 3 23 35 26 3 14 17 13 20 18 12 22 1 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "\n",
            "Post-softmax\n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 0.0213 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "mask = transformer.cross_attention_mask(sample_indices, sample_output_indices)\n",
        "for index_seq, mask in zip(sample_indices, mask):\n",
        "  print('Token sequence indices and output cross-attention mask matrix')\n",
        "  print_full(torch.cat([index_seq[None], mask]), digits='g')\n",
        "\n",
        "  print('Post-softmax')\n",
        "  print_full(torch.nn.functional.softmax(mask, dim=1), digits='.4f')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9PsqWbSpIpV"
      },
      "source": [
        "## Transformer Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "P8zZ6p1EpJ5b"
      },
      "outputs": [],
      "source": [
        "from typing import Callable, List\n",
        "import tqdm.notebook\n",
        "\n",
        "\n",
        "def average(a, average_window):\n",
        "  \"\"\"\n",
        "  :param a: an array\n",
        "  :param average_window: size of averaging window\n",
        "  :return: average over last averaging_window elements of a, or average\n",
        "  of a, if len(a) < average_window, or 0 if len(a) == 0\n",
        "  \"\"\"\n",
        "  return sum(a[-average_window:]) / max(1, min(len(a), average_window))\n",
        "\n",
        "\n",
        "class ProgressBarUpdater:\n",
        "  def __init__(\n",
        "      self, num_epochs: int | None, num_batches: int, average_window: int\n",
        "  ):\n",
        "    \"\"\"\n",
        "    :param num_epochs: total number of training epochs. None indicates that\n",
        "    batch mode is being used.\n",
        "    :param num_batches: total number of training batches\n",
        "    \"\"\"\n",
        "    tqdm_position = 0\n",
        "    if num_epochs is not None:\n",
        "      self.epoch_tqdm = tqdm.notebook.tqdm(\n",
        "          total=num_epochs, unit='epochs', position=tqdm_position\n",
        "      )\n",
        "      tqdm_position += 1\n",
        "    else:\n",
        "      self.epoch_tqdm = None\n",
        "\n",
        "    self.batch_tqdm = tqdm.notebook.tqdm(\n",
        "        total=num_batches, unit='batches', position=tqdm_position\n",
        "    )\n",
        "\n",
        "    self.average_window = average_window\n",
        "\n",
        "  def __call__(\n",
        "      self, i_epoch: int, i_batch: int,\n",
        "      losses: List[float], accuracies: List[float]\n",
        "  ):\n",
        "    \"\"\"\n",
        "    Called during training once per batch\n",
        "    :param i_epoch: current epoch index\n",
        "    :param i_batch: current batch index\n",
        "    :param losses: list of batch losses\n",
        "    :param accuracies: list of batch accuracies\n",
        "    \"\"\"\n",
        "    if self.epoch_tqdm is not None:\n",
        "      # update using epoch mode\n",
        "      if i_batch == 0 and i_epoch > 0:\n",
        "        self.batch_tqdm.reset()\n",
        "        self.epoch_tqdm.update(1)\n",
        "      else:\n",
        "        self.batch_tqdm.update(1)\n",
        "    else:\n",
        "      # update using batch mode\n",
        "      self.batch_tqdm.update(1)\n",
        "\n",
        "    self.batch_tqdm.set_postfix_str(\n",
        "        f'Current loss: {average(losses, self.average_window):.05f}, '\n",
        "        + 'Current accuracy: '\n",
        "        + f'{average(accuracies, self.average_window) * 100:.2f}%'\n",
        "    )\n",
        "\n",
        "\n",
        "class BatchProgressUpdater:\n",
        "  def __init__(\n",
        "      self, num_epochs: int | None, num_batches: int, average_window: int\n",
        "  ):\n",
        "    \"\"\"\n",
        "    :param num_epochs: total number of training epochs. None indicates that\n",
        "    batch mode is being used.\n",
        "    :param num_batches: total number of training batches\n",
        "    \"\"\"\n",
        "    self.num_epochs = num_epochs\n",
        "    self.num_batches = num_batches\n",
        "    self.average_window = average_window\n",
        "\n",
        "  def __call__(\n",
        "      self, i_epoch: int, i_batch: int,\n",
        "      losses: List[float], accuracies: List[float]\n",
        "  ):\n",
        "    \"\"\"\n",
        "    Called during training once per batch\n",
        "    :param i_epoch: current epoch index\n",
        "    :param i_batch: current batch index\n",
        "    :param losses: list of batch losses\n",
        "    :param accuracies: list of batch accuracies\n",
        "    \"\"\"\n",
        "    if self.num_epochs is not None:\n",
        "      epoch_str = f'Epoch {i_epoch + 1} / {self.num_epochs}. '\n",
        "    else:\n",
        "      epoch_str = ''\n",
        "    print(\n",
        "        epoch_str\n",
        "        + f'Batch {i_batch + 1} / {self.num_batches}. '\n",
        "        + f'Loss: {average(losses, self.average_window):.05f}. '\n",
        "        + f'Accuracy: {average(accuracies, self.average_window) * 100:.2f}%'\n",
        "    )\n",
        "\n",
        "\n",
        "class QATransformerTrainer:\n",
        "  def __init__(\n",
        "      self, folder, model, data_loader, optim, loss_fn, average_window\n",
        "  ):\n",
        "    \"\"\"\n",
        "    :param folder: The folder to save model parameters in\n",
        "    :param model: The model to train\n",
        "    :param data_loader: An object that provides batches of data\n",
        "    :param optim: optimizer object to use to train the model\n",
        "    :param loss_fn: the loss function, which computes loss given prob and\n",
        "    actual outputs of Transformer.forward\n",
        "    :param average_window: length of loss/accuracy averaging window\n",
        "    \"\"\"\n",
        "    self.folder = folder\n",
        "    self.model = model\n",
        "    self.data_loader = data_loader\n",
        "    self.optim = optim\n",
        "    self.loss_fn = loss_fn\n",
        "    self.average_window = average_window\n",
        "\n",
        "  def index_path(self, index: int):\n",
        "    \"\"\"\n",
        "    :param index: index to get the path for\n",
        "    :return: The file path of the given save index\n",
        "    \"\"\"\n",
        "    return os.path.join(self.folder, f'{index}.dat')\n",
        "\n",
        "  def save(self, index: int | None = None):\n",
        "    \"\"\"\n",
        "    :param index: index to save model to. If None, then a new index will\n",
        "    be created. Otherwise, the given index must exist in the save folder\n",
        "    \"\"\"\n",
        "    if index is not None:\n",
        "      path = self.index_path(index)\n",
        "      assert os.path.exists(path)\n",
        "    else:\n",
        "      files = [f for f in os.listdir(self.folder) if f.endswith('.dat')]\n",
        "      last_index = max(map(lambda f: int(os.path.splitext(f)[0]), files),\n",
        "                       default=-1)\n",
        "      path = self.index_path(last_index + 1)\n",
        "\n",
        "    torch.save(self.model.state_dict(), path)\n",
        "\n",
        "  def load(self, index: int):\n",
        "    \"\"\"\n",
        "    :param index: save index to load\n",
        "    Loads the model saved at the given index. Saves the current model\n",
        "    before overwriting it.\n",
        "    \"\"\"\n",
        "    path = self.index_path(index)\n",
        "    assert os.path.exists(path)\n",
        "\n",
        "    self.save()\n",
        "    self.model.load_state_dict(torch.load(path))\n",
        "\n",
        "  def train(\n",
        "      self,\n",
        "      epochs: int | None = None,\n",
        "      batches: int | None = None,\n",
        "      batch_callbacks: List[Callable[[int, int], None]] = (),\n",
        "      verbosity: int = 2\n",
        "  ):\n",
        "    \"\"\"\n",
        "    :param epochs: how many epochs to train for\n",
        "    :param batches: how many batches to train for (if epochs is None)\n",
        "    :param batch_callbacks: a list of function to call after each batch.\n",
        "    Each function should have signature\n",
        "    (i_epoch: int, i_batch: int) -> None, where i_epoch and\n",
        "    i_batch are the epoch and batch indices of the current epoch and batch\n",
        "    :param verbosity: How verbose the progress messsage should be.\n",
        "    0 = no progress messages, 1 = progress bars, 2 = one message per batch\n",
        "    If both epochs and batches are None, then the function will train for\n",
        "    1 epoch by default. If both are supplied, then batches is ignored.\n",
        "    \"\"\"\n",
        "\n",
        "    # default to 1 epoch if no arguments are given\n",
        "    if epochs is None and batches is None:\n",
        "      epochs = 1\n",
        "\n",
        "    # ignore batches argument in favor epochs\n",
        "    if epochs is not None:\n",
        "      batches = None\n",
        "\n",
        "    # input validation (avoid an almost infinite loop)\n",
        "    if epochs is not None:\n",
        "      assert epochs > 0\n",
        "\n",
        "    if batches is not None:\n",
        "      assert batches > 0\n",
        "\n",
        "    # training\n",
        "    # create progress updater\n",
        "    if verbosity == 0:\n",
        "      updater = None\n",
        "    elif verbosity == 1:\n",
        "      num_batches = len(self.data_loader) if batches is None else batches\n",
        "      updater = ProgressBarUpdater(\n",
        "          epochs, num_batches, self.average_window\n",
        "      )\n",
        "    elif verbosity == 2:\n",
        "      num_batches = len(self.data_loader) if batches is None else batches\n",
        "      updater = BatchProgressUpdater(\n",
        "          epochs, num_batches, self.average_window\n",
        "      )\n",
        "\n",
        "    # store losses and accuracies and return them\n",
        "    batch_losses, batch_accuracies = [], []\n",
        "\n",
        "    # put model in training mode\n",
        "    self.model.train()\n",
        "\n",
        "    # iterate over desired number of epochs or forever if using batches\n",
        "    for i_epoch in range(999999999999999 if epochs is None else epochs):\n",
        "      # self.data_loader is an iterable of all batches in an epoch\n",
        "      for i_batch, (b_question, b_answer) in enumerate(self.data_loader):\n",
        "        # exit if we have done enough batches\n",
        "        if batches is not None and len(batch_losses) == batches:\n",
        "          return batch_losses, batch_accuracies\n",
        "\n",
        "        # get next-token probabilities and actual indices\n",
        "        prob, actual = self.model(b_question, b_answer)\n",
        "\n",
        "        # compute loss + gradients, and perform optimization step\n",
        "        loss = self.loss_fn(prob, actual)\n",
        "\n",
        "        self.optim.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optim.step()\n",
        "\n",
        "        # save batch loss and accuracy\n",
        "        batch_losses.append(loss.item())\n",
        "        not_pads = (actual != self.model.token_embedding.pad_index).float()\n",
        "        n_tokens = not_pads.sum(dim=1)\n",
        "        matched = (prob.argmax(dim=1) == actual).float() * not_pads\n",
        "        acc = (matched.sum(dim=1) / n_tokens).mean()\n",
        "        batch_accuracies.append(acc)\n",
        "\n",
        "        # print progress messages and run callbacks\n",
        "        if updater is not None:\n",
        "          updater(i_epoch, i_batch, batch_losses, batch_accuracies)\n",
        "\n",
        "        for batch_callback in batch_callbacks:\n",
        "          batch_callback(i_epoch, i_batch)\n",
        "\n",
        "    return batch_losses, batch_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "a32MBYJgUTl9"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "\n",
        "class SavePeriodicallyCallback:\n",
        "  \"\"\"\n",
        "  Type of callback to be passed to QATransformerTrainer.train's\n",
        "  batch_callbacks option. This callback will save the model being\n",
        "  every time a specified amount of time since the last save elapses.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, trainer: QATransformerTrainer, save_interval: float):\n",
        "    \"\"\"\n",
        "    :param trainer: QATransformerTrainer trainer object training the model\n",
        "    :param save_interval: Time in seconds between model saves.\n",
        "    \"\"\"\n",
        "    self.trainer = trainer\n",
        "    self.save_interval = save_interval\n",
        "    self.last_save_time = time.time()\n",
        "\n",
        "  def __call__(self, i_epoch, i_batch):\n",
        "    \"\"\"\n",
        "    Saves the model if self.save_interval seconds have elapsed since\n",
        "    the last save\n",
        "    :param i_epoch: current epoch index\n",
        "    :param i_batch: current batch index\n",
        "    \"\"\"\n",
        "    if time.time() - self.last_save_time >= self.save_interval:\n",
        "      self.trainer.save()\n",
        "      self.last_save_time = time.time()\n",
        "\n",
        "\n",
        "class WarmupLRScheduleCallback:\n",
        "  \"\"\"\n",
        "  Callback to be passed to the callbacks option of\n",
        "  QATransformerTrainer.train. This callback sets the learning rate of the\n",
        "  optimizer according the schedule in Vaswani et al. (section 5.3)\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, warmup_steps: int, d: int, optim, base_lr: float = 1.0):\n",
        "    \"\"\"\n",
        "    :param warmup_steps: number of warmup steps\n",
        "    :param d: embedding dimension of model\n",
        "    :param optim: PyTorch optimizer object being used to optimize\n",
        "    the model\n",
        "    :param base_lr: Base learning rate to multiply by\n",
        "    \"\"\"\n",
        "    self.warmup_steps = warmup_steps\n",
        "    self.d = d\n",
        "    self.optim = optim\n",
        "    self.base_lr = base_lr\n",
        "    self.step = 0\n",
        "\n",
        "  def __call__(self, i_epoch: int, i_batch: int):\n",
        "    \"\"\"\n",
        "    Advances the step counter and updates the learning rate of the\n",
        "    optimizer according to schedule in Vaswani et al.\n",
        "    :param i_epoch: current epoch index\n",
        "    :param i_batch: current batch index\n",
        "    \"\"\"\n",
        "    self.step += 1\n",
        "    self.optim.lr = self.base_lr * self.d**(-.5) * min(\n",
        "        self.step**(-.5), self.step * self.warmup_steps**(-1.5)\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "4TxlYxEyUosL"
      },
      "outputs": [],
      "source": [
        "model = Transformer(\n",
        "    TokenEmbedding(arithmetic_easy_tokens, 512),\n",
        "    30, 6, 6, 8, 2048, p_dropout=.1\n",
        ").cuda()\n",
        "\n",
        "ar_easy_dl = torch.utils.data.DataLoader(\n",
        "    arithmetic_easy, batch_size=128, shuffle=True,\n",
        ")\n",
        "\n",
        "optim = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=6e-4,\n",
        "    betas=(.9, .995),\n",
        "    eps=1e-9\n",
        ")\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(\n",
        "    ignore_index=model.token_embedding.pad_index, label_smoothing=.05\n",
        ")\n",
        "\n",
        "trainer = QATransformerTrainer('model2', model, ar_easy_dl, optim, loss_fn, 100)\n",
        "\n",
        "# save every 900s = 15min\n",
        "save_callback = SavePeriodicallyCallback(trainer, 900)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pT7ZOr5GUqRB",
        "outputId": "080f75b1-7e9e-434d-a394-2cbcc2f4dd79"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5ea197dc94a44c18dce16ddbc90f006",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?epochs/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "526ad8daf3f14bb5af534215b97a6e5c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/23438 [00:00<?, ?batches/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "losses, accuracies = trainer.train(\n",
        "    epochs=2,\n",
        "    batch_callbacks=[save_callback],\n",
        "    verbosity=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.load(12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch = next(iter(ar_easy_dl))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.train()\n",
        "prob, actual = model(batch[0], batch[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([256, 47, 22])"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prob.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([256, 22])"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "actual.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In base 9, what is 442 + -5?\n",
            "tensor([-1.5149, -0.6246, -1.5795, -1.5071, -1.6892, -1.6505, -1.7726, -1.8778,\n",
            "        -1.5867,  4.0158, -0.7512, -1.4037,  1.8952,  3.3266,  2.7380,  2.3791,\n",
            "         2.2658,  2.2600,  1.6691,  1.7582,  1.4803,  1.2695, -1.6022, -1.0796,\n",
            "        -1.2574, -1.1826, -1.4925, -1.2410, -1.6388, -1.5754, -1.7663, -1.6779,\n",
            "        -1.5637, -1.5802, -1.6068, -1.5572, -1.6538, -1.6904, -1.5834, -1.7714,\n",
            "        -1.5040, -1.6266, -1.6930, -1.6779, -1.7156, -1.5514, -1.5940],\n",
            "       device='cuda:0')\n",
            "tensor([-0.8981,  3.9388, -0.9819, -0.6913, -1.4301, -1.1849, -2.3033, -0.8399,\n",
            "        -1.1060,  0.1913,  1.3431, -1.1932, -0.7594,  0.9131,  1.1400,  1.6206,\n",
            "         0.8799,  1.4027,  0.8400,  1.1394, -0.5853, -0.0760, -0.9776, -2.2735,\n",
            "        -1.5882, -1.6100, -1.6587, -0.9358, -1.3828, -0.9994, -1.3601, -1.2240,\n",
            "        -0.9191, -1.2290, -0.9951, -0.9989, -1.4705, -1.0713, -0.7547, -1.5517,\n",
            "        -0.8162, -1.0285, -0.9250, -1.1235, -0.7338, -0.9410, -1.1143],\n",
            "       device='cuda:0')\n",
            "-\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(batch[0][50])\n",
        "    print(model(batch[0][50]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 9, 10, 10, 20,  1]], device='cuda:0')\n",
            "tensor([[14, 17, 10, 18,  1]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "model.train()\n",
        "prob, actual = model(batch[0][20:21], batch[1][20:21])\n",
        "print(prob.argmax(dim=1))\n",
        "print(actual)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
