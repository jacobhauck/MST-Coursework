\documentclass{homework}
\input{../homework_shared.tex}

\newcommand{\hwnum}{1}
\renewcommand{\questiontype}{Problem}

\begin{document}
\maketitle

\question
\begin{alphaparts}
	\questionpart See \verb*|bisect.m| -- also copied here for convenience.
	\lstinputlisting[language=MATLAB, numbers=left, frame=single, basicstyle=\small\ttfamily, showstringspaces=false]{bisect.m}
	\questionpart \begin{enumerate}[label=\textbf{(\arabic*)}]
		\item The following are snippets from the relevant parts of \verb*|outputs.txt|, which contains the output from the MATLAB console. For $\varepsilon = 10^{-2}$:
		\lstinputlisting[basicstyle=\small\ttfamily, frame=single, firstline=1, lastline=16]{outputs.txt}
		For $\varepsilon = 10^{-4}$:
		
		\lstinputlisting[basicstyle=\small\ttfamily, frame=single, firstline=17, lastline=38]{outputs.txt}
		For $\varepsilon = 10^{-8}$:
		\lstinputlisting[basicstyle=\small\ttfamily, frame=single, firstline=40, lastline=74]{outputs.txt}
		\item The maximum error $M_k$ after $k$ iterations of the bisection method is given by \begin{equation}
			M_k = \frac{b-a}{2^{k+1}}
		\end{equation}
		To obtain a maximum error less than $\varepsilon > 0$, we need that $k$ satisfies the inequality
		\begin{equation}
			M_k < \varepsilon \iff \frac{b-a}{\varepsilon} < 2^{k+1}
		\end{equation}
		Thus, we need
		\begin{equation}
			\label{eq:bisection_iteration_lower_bound}
			k > \log_2\left(\frac{b-a}{2\varepsilon}\right)
		\end{equation}
		Since $k$ must be an integer, the least number of iterations needed to guarantee an error no greater than $\varepsilon$ is given by the ceiling of the left side of (\ref{eq:bisection_iteration_lower_bound}), that is, the smallest integer greater than or equal to RHS(\ref{eq:bisection_iteration_lower_bound}):
		\begin{equation}
			k = \left\lceil \log_2\left(\frac{b-a}{2\varepsilon}\right) \right\rceil
		\end{equation}
		For $[a,b] = [-4.9,5.1]$ and $\varepsilon = 10^{-2}$, this gives $k = \lceil 8.9658 \rceil = 9$; for $\varepsilon = 10^{-4}$, it gives $k = \lceil 15.6096\rceil = 16$; and for $\varepsilon = 10^{-8}$ it gives $k=\lceil 28.8974\rceil = 29$. These are exactly the number of iterations that were executed in the numerical experiments.
	\end{enumerate}
	
\end{alphaparts}

\question

\begin{alphaparts}
	\questionpart See \verb*|fixed.m| -- also copied here for convenience.
	\lstinputlisting[language=MATLAB, numbers=left, frame=single, basicstyle=\small\ttfamily, showstringspaces=false]{fixed.m}
	The following are relevant snippets from \verb*|outputs.txt|, which contains the output from the MATLAB console. For $x_0 = 5$:
	\lstinputlisting[basicstyle=\small\ttfamily, frame=single, firstline=76, lastline=91]{outputs.txt}
  	For $x_0 = -5$:
  	\lstinputlisting[basicstyle=\small\ttfamily, frame=single, firstline=93, lastline=108]{outputs.txt}
  	For $x_0 = 1$:
  	\lstinputlisting[basicstyle=\small\ttfamily, frame=single, firstline=110, lastline=125]{outputs.txt}
  	For $x_0 = -1$:
  	\lstinputlisting[basicstyle=\small\ttfamily, frame=single, firstline=127, lastline=142]{outputs.txt}
  	For $x_0 = 0.1$: 
	\lstinputlisting[basicstyle=\small\ttfamily, frame=single, firstline=144, lastline=159]{outputs.txt}
	
	\questionpart It appears that the algorithm converges to $0$ from all the initial guesses that I experimented with. The ones that start closer to $0$ get closer to $0$ in fewer iterations than the ones that start farther from $0$.
	
	First, set $G = [-R, R]$, where $R > 0$ is large enough that $x_0 \in [-R, R]$. By the Fundamental Theorem of Calculus, if $x \in G$, then
	\begin{equation}
		|g(x)| = \left|x - \tan^{-1}(x)\right| = \left|\int_0^x\left(1 - \frac{1}{1+t^2}\right)\;\text{d}t\right| \le |x| \le R
	\end{equation}
	Therefore, $g(G) \subseteq G$. Furthermore, $g$ is $L$-Lipschitz on $[-R, R]$ with $L = 1 - \frac{1}{1+R^2} < 1$ because
	\begin{equation}
		g'(x) = 1-\frac{1}{1+x^2} \le 1-\frac{1}{1+R^2}
	\end{equation}
	if $x \in G$. Therefore, $g$ is a contraction on $G$, so the fixed point method must converge for any $x_0 \in G$. Since $G = [-R,R]$, and $R > 0$ was arbitrary, it follows that the fixed point method should converge for all initial guesses.
	
	Second, if the initial guess $x_0$ is farther from the fixed point $z=0$, then the error bound
	\begin{equation}
		|x_k - z| \le \frac{L^k}{1-L}|x_1 - x_0|
	\end{equation}
	is looser as $L$ gets bigger, and we need to choose a bigger $L$ when $x_0$ is farther from $0$ because we need to choose $R$ large enough so that $x_0 \in [-R, R]$ in order for the fixed point theorem to apply with the initial guess $x_0$. The looser bound for $x_0$ farther from $0$ suggests that the algorithm will require more iterations when $x_0$ is farther from $0$.
\end{alphaparts}

\question
First, we need to show that $g(G) \subseteq G$. Note that
\begin{equation}
	g'(x) = \frac{1}{3}\left(x^2 - 2x -\frac{5}{4}\right), \qquad g''(x) = \frac{2}{3}(x - 1)
\end{equation}
The roots of $g'$ are $1 \pm \frac{1}{2}\sqrt{4 + 5} = 1 \pm \frac{3}{2}$, and the only root of $g''$ is $1$. Since $1\pm \frac{3}{2} \notin [0,2]$, the Extreme Value Theorem implies that
\begin{gather}
	\max_{x\in G} g(x) = \max\{g(0), g(2)\} = \max\left\{\frac{4}{3}, \frac{1}{18}\right\} = \frac{4}{3}\\
	\min_{x\in G} g(x) = \min\{g(0), g(2)\} = \min\left\{\frac{4}{3},\frac{1}{18}\right\} = \frac{1}{18}
\end{gather}
Therefore, $g(G) \subseteq \left[\frac{1}{18},\frac{4}{3}\right]\subseteq G$. Furthermore, the Extreme Value Theorem also implies that
\begin{gather}
	\max_{x\in G} g'(x) = \max\{g'(0), g'(2), g'(1)\} = \max\left\{-\frac{5}{12},-\frac{9}{12}\right\} = -\frac{5}{12} \\
	\min_{x\in G} g'(x) = \min\{g'(0), g'(2), g'(1)\} = \min\left\{-\frac{5}{12},-\frac{9}{12}\right\} = -\frac{9}{12}
\end{gather}
Therefore, $|g'| \le \frac{9}{12}$ on $G$, so $g$ is $L$-Lipschitz on $G$ with $L = \frac{9}{12} < 1$. By the Contraction Mapping Theorem, there is a unique fixed point $z$ of $g$ on $G$, and for any $x_0 \in G$, the sequence $\{x_k\}_{k=0}^\infty$ defined recursively by $x_{k+1} = g(x_k)$ converges to $z$ as $k \to \infty$.
\end{document}