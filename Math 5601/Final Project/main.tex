\documentclass{homework}
\input{../homework_shared.tex}
\input{../../standardcmd.tex}

\usepackage{float}
\usepackage{booktabs}

\renewcommand{\hwtype}{Final Project}
\newcommand{\hwnum}{}
\renewcommand{\questiontype}{Problem}


\begin{document}
	\maketitle
	
	Consider the following second-order ODE with Dirichlet boundary conditions:
	\begin{align}
		\label{eq:ode}
		\dl{}{x}\left(c(x)\dl{u(x)}{x}\right) &= f(x),\qquad a \le x \le b, \\
		\label{eq:bc}
		u(a) = g_a,\quad u(b) &= g_b.
	\end{align}
	
	\question
	Consider the second-order ODE (\ref{eq:ode}). Multiplying by $v \in H^1([a,b])$ and integrating by parts gives
	\begin{equation}
		\label{eq:ibp}
		\int_a^bfv = c(b)u'(b)v(b) - c(a)u'(a)v(a) - \int_a^b cu'v.
	\end{equation}
	
	\begin{alphaparts}
		\questionpart Suppose we have the boundary conditions
		\begin{equation}
			u'(a) = p_a, \qquad u(b) = g_b.
		\end{equation}
		Equation (\ref{eq:ibp}) still holds, and we can impose the condition $v(b) = 0$ because we already know that $u(b) = p_b$. Since $u'(a) = p_a$, equation (\ref{eq:ibp}) becomes
		\begin{equation}
			\int_a^b fv = -c(a)p_av(a) - \int_a^bcu'v'
		\end{equation}
		for all $v \in H^1([a,b])$ such that $v(b) = 0$, which is our weak formulation of (\ref{eq:ode}) with the given boundary conditions.
		
		\questionpart Suppose we have the boundary conditions
		\begin{equation}
			u'(a) = p_a, \qquad u'(b) + q_bu(b) = p_b.
		\end{equation}
		Equation (\ref{eq:ibp}) still holds. Since $u'(b) = p_b - q_bu(b)$, and $u'(a) = p_a$, we get
		\begin{equation}
			\int_a^bfv = c(b)(p_b - q_bu(b))v(b) - c(a)p_av(a) - \int_a^bcu'v'
		\end{equation}
		for all $v \in H^1([a,b])$, which is our weak formulation of (\ref{eq:ode}) with the given boundary conditions.
		
		\questionpart Suppose we have the boundary conditions
		\begin{equation}
 			u'(a) = p_a,\qquad u'(b) = p_b.
		\end{equation}
		Equation (\ref{eq:ibp}) still holds. Since $u'(a) = p_a$, and $u'(b) = p_b$, we get
		\begin{equation}
			\label{eq:weak_neumann}
			\int_a^bfv = c(b)p_bv(b) - c(a)p_av(a) - \int_a^bcu'v'
		\end{equation}
		for all $v \in H^1([a,b])$, which is our weak formulation of (\ref{eq:ode}) with the given boundary conditions.
		
		We note that solutions of this formulation are not unique. Indeed, if $u \in H^1([a,b])$ satisfies (\ref{eq:weak_neumann}) for all $v \in H^1([a,b])$, then so does $u + \alpha$, where $\alpha \in \R$ is any real number because $(u+\alpha)' = u'$ regardless of what $\alpha$ is, and the weak formulation depends only on $u'$.
	\end{alphaparts}
	
	\question Consider the Poisson equation
	\begin{equation}
		\label{eq:poisson}
		\nabla \cdot (c\nabla u) = f \; \text{in}\; D.
	\end{equation}
	Using integration by parts, we have
	\begin{equation}
		\label{eq:ibp2}
		\int_D fv = \int_D \nabla \cdot(c\nabla u)v = \int_{\partial D} cv \nabla u \cdot n \dee S - \int_D c\nabla u \cdot \nabla v,
	\end{equation}
	where $\text{d}S$ is the surface measure on $\partial D$, and $v \in H^1\left(\overline{D}\right)$.
	
	\begin{alphaparts}
		\questionpart Suppose that we have the boundary condition
		\begin{equation}
			u = g \;\text{on}\;\partial D.
		\end{equation}
		Equation (\ref{eq:ibp2}) still holds. Since we know the value of $u$ on $\partial D$, we can set $v = 0$ on $\partial D$. Then we get
		\begin{equation}
			\int_D fv = - \int_D c\nabla u\cdot \nabla v
		\end{equation}
		for all $v \in H^1\left(\overline{D}\right)$ such that $v = 0$ on $\partial D$, which is our weak formulation of (\ref{eq:poisson}) with the given boundary condition.
		
		\questionpart Suppose that we have the boundary condition
		\begin{equation}
			\nabla u\cdot n + qu = p \;\text{on}\;\partial D,
		\end{equation}
		where $n$ is the outward unit normal vector to $\partial D$, and $p$ and $q$ are functions on $\partial D$. Equation (\ref{eq:ibp2}) still holds. Since $\nabla u \cdot n = p - qu$ on $\partial D$, it follows that
		\begin{equation}
			\int_D fv = \int_{\partial D} cv(p-qu)\dee S - \int_D c\nabla u\cdot\nabla v
		\end{equation}
		for all $v \in H^1\left(\overline{D}\right)$, which is our weak formulation of (\ref{eq:poisson}) with the given boundary condition.
	\end{alphaparts}
	
	\question
	If $u \in C^2[a,b]$, then
	\begin{align}
		\label{eq:lin_interp_value}
		\lVert u - I_hu\rVert_\infty &\le \frac{1}{8}h^2\lVert u'' \rVert_\infty,\\
		\label{eq:lin_interp_deriv}
		\lVert (u - I_hu)'\rVert_\infty &\le \frac{1}{2}h\lVert u''\rVert_\infty.
	\end{align}
	\begin{proof}
		Consider the interval $[x_i, x_{i+1}]$, where $1 \le i \le N$. Restricted to this interval, $I_hu$ is the degree-1 Lagrange polynomial interpolation of $u$ on with nodes $x_i$ and $x_{i+1}$. By the error formula for Lagrange polynomial approximation in the slides,
		\begin{equation}
			u(x) - I_hu(x) = \frac{f''(\xi(x))(x-x_i)(x-x_{i+1})}{2}
		\end{equation}
		for some $\xi(x) \in [x_i,x_{i+1}]$. Then
		\begin{equation}
			|u(x) - I_hu(x)| \le \lVert f''\rVert_\infty \cdot \frac{1}{2}(x-x_i)(x_{i+1}-x).
		\end{equation}
		The function $g(x) = (x-x_i)(x_{i+1} -x)$ is a downward-opening parabola, so it achieves maximum halfway between its roots $x_i$ and $x_{i+1}$. Therefore,
		\begin{align}
			|u(x) - I_hu(x)| &\le \lVert f'' \rVert_\infty \cdot\frac{\left(\frac{x_i + x_{i+1}}{2} - x_i\right)\left(x_{i+1}-\frac{x_i + x_{i+1}}{2}\right)}{2} \\
			&=  \lVert f''\rVert_\infty \frac{(x_{i+1} - x_i)^2}{8} = \frac{h^2}{8}\lVert f''\rVert_\infty.
		\end{align}
		Since this holds for all $x \in [x_i, x_{i+1}]$ and all $1\le i \le N$, it holds for all $x \in [a,b]$. Therefore, the inequality (\ref{eq:lin_interp_value}) follows.
		
		Let $1 \le i \le N$, and let $x \in [x_i, x_{i+1}]$. By the Mean Value Theorem, there exists $ c\in [x_i, x_{i+1}]$ such that
		\begin{equation}
			u'(c) = \frac{u(x_{i+1}) - u(x_i)}{x_{i+1} - x_i} = (I_hu)'(x).
		\end{equation}
		Then $(I_hu)'$ is the degree-0 Lagrange polynomial interpolation of $u'$ with node $c$. By the error formula for Lagrange polynomial approximation in the slides,
		\begin{equation}
			u'(x) - (I_hu)'(x) = u''(\xi(x))(x-c)
		\end{equation}
	\end{proof}
	
	\question 
	Consider the weak formulation of
	\begin{equation}
		\nabla\cdot(c\nabla u) = f \;\text{in}\; D,\qquad u = g \;\text{on}\;\partial D
	\end{equation}
	derived in problem 2 (a):
	\begin{equation}
		\label{eq:poisson_weak}
		\int_D fv = -\int_D c\nabla u\cdot \nabla v
	\end{equation}
	for all $v \in H^1\left(\overline{D}\right)$ such that $v = 0$ on $\partial D$. Suppose that we have basis functions $\{\phi_i\}_{i=1}^{N+1}$ for a finite element space $U_h$ on $\overline{D}$. To approximate a solution of the weak formulation, we approximate $H^1$ by $U_h$. Thus, we want to find $u \in U_h$ such that (\ref{eq:poisson_weak}) holds for all $v \in U_h$.
	
	By the linearity of the problem and the fact that $U_h = \mathrm{span}\{\phi_i\}$, this is equivalent to (\ref{eq:poisson_weak}) being true for $v = \phi_i$, for $i = 1, \dots, N+1$. Since we want $u \in U_h$, there exist coefficients $u_j$ such that
	\begin{equation}
		u = \sum_{j=1}^{N+1}u_j\phi_j.
	\end{equation}
	Hence, we need
	\begin{equation}
		\int_D f\phi_i = -\int_D c\nabla \left(\sum_{j=1}^{N+1}u_j\phi_j\right)\cdot \nabla \phi_i
	\end{equation}
	for all $i = 1,\cdots,N+1$. Using the linearity of $\nabla$ and rearranging terms, this is equivalent to
	\begin{equation}
		\sum_{j=1}^{N+1}u_j\left[-\int_Dc\nabla \phi_j\cdot\nabla \phi_i\right] = \int_D f\phi_i
	\end{equation}
	for all $i =1, \dots, N+1$. If we set
	\begin{equation}
		A_{ij} = -\int_Dc\nabla\phi_j\cdot\nabla\phi_i,\qquad b_i = \int_D f\phi_i, \qquad X_j = u_j,
	\end{equation}
	then this is equivalent to the linear system $AX = b$.
	
	\question
	Let $A$ be a nonsingular, lower-triangular matrix; that is, $i < j$ implies that $A_{ij} = 0$. Then $A^{-1}$ is also lower-triangular.
	
	\begin{proof}
		We use induction on the size of the matrix. All $1\times 1$ matrices are trivially lower-triangular, so the base case holds. Now suppose that the claim is true for all matrices of size $n \times n$, where $n \ge 1$.
		
		Let $A$ be a nonsingular, $(n+1)\times(n+1)$, lower-triangular matrix. Then every entry but the last entry of the last column of $A$ is zero by the lower-triangular condition. That is, we can write $A$ in block matrix form as
		\begin{equation}
			A = \left[\begin{matrix}
				B & 0 \\
				c & d
			\end{matrix}\right],
		\end{equation}
		where $B$ is a $n\times n$ matrix, $c$ is a $1\times n$ row vector, and $d$ is a scalar. Since $A_{ij} = B_{ij}$ if $i,j \le n$, it follows that $B$ is also lower-triangular. Furthermore, $B$ must be nonsingular. 
		
		Indeed, suppose for the sake of contradiction that $B$ is singular. Then its rows $\{B_1,\cdots, B_n\}$ are linearly dependent. That is, there exist $\alpha_1, \dots, \alpha_n$ not all zero such that
		\begin{equation}
			\alpha_1 B_1 + \cdots + \alpha_nB_n = 0.
		\end{equation}
		Let $\{A_1, \cdots, A_n, A_{n+1}\}$ denote the rows of $A$. Then $A_i = \left[\begin{matrix}B_i & 0\end{matrix}\right]$ for $1 \le i \le n$. Hence,
		\begin{equation}
			\alpha_1 A_1 + \cdots + \alpha_n A_n = 0
		\end{equation}
		as well. This implies that the rows of $A$ are linearly dependent, which contradicts the nonsingularity of $A$.
		
		Therefore, $B$ is a nonsingular, $n\times n$, lower-triangular matrix, and the induction hypothesis implies that $B^{-1}$ is lower-triangular.
		
		In addition, $d\ne0$ because $d=0$ implies that $\det(A) = 0$ upon expansion by cofactors on the last column of $A$, which contradicts the nonsingularity of $A$.
		
		We now observe that
		\begin{equation}
			A\left[\begin{matrix}
				B^{-1} & 0 \\
				-cB^{-1}d^{-1} & d^{-1}
			\end{matrix}\right] = \left[\begin{matrix}
			B & 0 \\
			c & d
			\end{matrix}\right]\left[\begin{matrix}
			B^{-1} & 0 \\
			-cB^{-1}d^{-1} & d^{-1}
			\end{matrix}\right] = \left[\begin{matrix}
				I_{n\times n} & 0 \\
				cB^{-1} - cB^{-1}d^{-1}d & 1
			\end{matrix}\right] = I_{(n+1)\times(n+1)},
		\end{equation}
		so
		\begin{equation}
			A^{-1} = \left[\begin{matrix}
				B^{-1} & 0 \\
				-cB^{-1}d^{-1} & d^{-1}
			\end{matrix}\right].
		\end{equation}
		Then $A^{-1}$ is lower-triangular because $B^{-1}$ is lower triangular.
	\end{proof}
	
	\question 
	Let
	\begin{equation}
		A = \left[\begin{matrix}
			\kappa & \lambda \\
			\lambda & \mu
		\end{matrix}\right]
	\end{equation}
	be a positive definite matrix. Then the Jacobi method for $Ax = b$ converges.
	
	\begin{proof}
		We recall from the slides that the Jacobi method is the iteration
		\begin{equation}
			x^{(k+1)} = -D^{-1}Nx^{(k)} + D^{-1}b,
		\end{equation}
		where $D$ is the diagonal of $A$, and $N$ is the off-diagonal of $A$. This iteration converges if and only if $\rho(-D^{-1}N) < 1$. In this case,
		\begin{equation}
			-D^{-1}N = -\left[\begin{matrix}
				0 & \frac{\lambda}{\mu} \\
				\frac{\lambda}{\kappa} & 0
			\end{matrix}\right],
		\end{equation}
		so any eigenvalue $\rho$ of $-D^{-1}N$ satisfies $\rho^2 - \frac{\lambda^2}{\kappa\mu} = 0$. Therefore $|\rho| < 1$ if and only if $\lambda^2 < \kappa\mu$, or $\kappa \mu  -\lambda^2 > 0$. Since $\kappa\mu - \lambda^2 = \det(A)$, and the positive definiteness of $A$ implies that $\det(A) > 0$, it follows that $\rho(-D^{-1}N) < 1$, and the Jacobi method converges.
	\end{proof}
	
	\question
	\begin{alphaparts}
		\questionpart 
		\questionpart 
	\end{alphaparts}
	
	\question 
	\begin{alphaparts}
		\questionpart
	\end{alphaparts}
\end{document}