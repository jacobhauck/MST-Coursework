\documentclass{homework}\input{../../standardcmd.tex}\input{../homework_shared.tex}\usepackage{listings}\newcommand{\hwnum}{3}\renewcommand{\questiontype}{Problem}\begin{document}	\maketitle		\question		\begin{alphaparts}		\questionpart We need to calculate $P(N(2) = 0)$ because the event that no customers come in the first two hours (8am to 10am) is the same as $N(2) = 0$. Since $N(2)$ is Poisson distributed with mean $2\cdot 3 = 6$, we have		\begin{equation*}			P(N(2) = 0) = e^{-6}.		\end{equation*}				\questionpart Let $W$ be the amount of time Oscar has to wait for a customer to arrive. If any customers have arrived already, then Oscar doesn't have to wait at all, so $W = 0$ given $N(2) > 0$. If no customers have arrived when Oscar starts work at 10am, then the amount of time he has to wait for a customer to arrive is exponentially distributed with rate 3 (due to the memoryless property of Poisson process event times). That is, $W$ is exponential with rate 3 given that $N(2) = 0$. Thus, by part (a),		\begin{align*}			F(t) = P(W \le t) &= P(W \le t | N(2) = 0)P(N(2) = 0) + P(W \le t| N(2) > 0)P(N(2) > 0) \\			&= (1-e^{-3t})e^{-6} + 1\cdot(1-e^{-6}) \\			&= 1 - e^{-3t-6}, \qquad t \ge 0		\end{align*}		is the CDF of $W$.	\end{alphaparts}		\question N/A -- already did this one in Homework 2.		\question 		\begin{alphaparts}		\questionpart		If $N(t)$ is the total number of signals transmitted up to time $t$, then $N_1(t)$ and $N_2(t)$ are obtained from $N(t)$ by thinning; therefore, $\{N_(t) : t\ge 0\}$ and $\{N_2(t) : t \ge 0\}$ are independent Poisson processes with rates $\lambda_1 = p\lambda$ and $\lambda_2 = (1-p)\lambda$. Thus, $(N_1(t), N_2(t))$ are independent Poisson random variables with means $p\lambda t$ and $(1-p)\lambda t$, so the joint PMF is given by		\begin{equation*}			f(n_1, n_2) = \frac{(p\lambda t)^{n_1}((1-p)\lambda t)^{n_2}e^{-p\lambda}e^{-(1-p)\lambda}}{n_1!n_2!} = 	\frac{p^{n_1}(1-p)^{n_2}(\lambda t)^{n_1+n_2}e^{-\lambda}}{n_1!n_2!}, \qquad n_1\ge 0,\, n_2\ge 0.		\end{equation*}				\questionpart We note that for $k > 0$, we have $L \ge k$ if and only if $S_k^2 < S_1^1$, where $S_n^j$ is the time until the $n$th signal is transmitted if $j=1$ and lost if $j=2$. By the Poisson race formula,		\begin{equation*}			P(L \ge k) = P(S_k^2 < S_1^1) = \sum_{i=k}^k \binom{k}{i} (1-p)^i p^{k-i} = (1-p)^k, \qquad k > 0.		\end{equation*}		Also, $P(L= 0)$ is the probability that the first signal is successfully transmitted, which is given as $p$. Then the PMF of $L$ is given by		\begin{equation*}			P(L=k) = P(L\ge k) - P(L\ge k+1) = (1-p)^k - (1-p)^{k+1} = (1- (1-p))(1-p)^k = p(1-p)^k, \qquad k \ge 0,		\end{equation*}		if we note that $P(L=0) = p = p(1-p)^0$, making the above also correct for $k=0$. This is the PMF of a geometric random variable with parameter $p$.	\end{alphaparts}			\question 	\begin{alphaparts}		\questionpart For $u > 0$, using the independence of the $Y_i$ from each other and from $N(t)$, we have		\begin{align*}			f(u) = E\left[u^S\right] &= E\left[u^{\sum\limits_{i=1}^{N(t)}Y_i}\right] = E\left[ E\left[\prod_{i=1}^{N(t)}u^{Y_i} \bigg\vert N(t)\right]\right]\\			&= \sum_{n=0}^\infty E\left[\prod_{i=1}^n u^{Y_i}\right]P(N(t) = n) \\			&= \sum_{n=0}^\infty \frac{(\lambda t)^n e^{-\lambda t}}{n!} \prod_{i=1}^n E[u^{Y_i}] \\			&= \sum_{n=0}^\infty \frac{(\lambda t)^n e^{-\lambda t}}{n!} (g(u))^n \\			&= e^{-\lambda t + \lambda t g(u)} \sum_{n=0}^\infty \frac{(\lambda t g(u))^ne^{-\lambda t g(u)}}{n!} \\			&= e^{\lambda t (g(u) - 1)}.		\end{align*}				\questionpart Differentiating $f$ gives		\begin{equation*}			f'(u) = \lambda t g'(u)e^{\lambda t (g(u) - 1)},		\end{equation*}		and		\begin{equation*}			f''(u) = \left[\lambda t g''(u)+ (\lambda t g'(u))^2\right]e^{\lambda t (g(u)-1)}.		\end{equation*}		Then 		\begin{equation*}			E[S] = f'(1) = \lambda t g'(1)e^{\lambda t (g(1) - 1)} = \lambda t E[Y_i],		\end{equation*}		and		\begin{align*}			\Var[S] &= E[S^2] - E[S]^2 = E[S(S-1)] + E[S] - E[S]^2 \\			&= f''(1) + \lambda t E[Y_i] - \lambda^2t^2 E[Y_i]^2 \\			&= \lambda t g''(1) + (\lambda t g'(1))^2 + \lambda tE[Y_i] - \lambda^2 t^2 E[Y_i]^2 \\			&= \lambda t E[Y_i^2] - \lambda t E[Y_i] + \lambda^2t^2 E[Y_i]^2 + \lambda t E[Y_i] - \lambda^2 t^2 E[Y_i]^2\\			&= \lambda t (\Var[Y_i] + E[Y_i]^2).		\end{align*}	\end{alphaparts}		\question We want to show that the two following definitions are equivalent.	\begin{enumerate}		\item A counting process $\{N(t) : t \ge 0\}$ is a Poisson process with rate $\lambda > 0$ if		\begin{enumerate}			\item $N(0) = 0$,			\item the process has independent increments,			\item $P(N(t+h) - N(t)=1) = \lambda h + o(h)$ for any $t \ge 0$,			\item and $P(N(t+h) - N(t)\ge 2) = o(h)$ for any $t \ge 0$.		\end{enumerate}		\item A counting process $\{N(t) : t \ge 0\}$ is a Poisson process with rate $\lambda > 0$ if		\begin{enumerate}			\item $N(0) = 0$,			\item the process has independent increments,			\item $N(t+s) - N(t)$ is Poisson-distributed with mean $\lambda s$ for any $t \ge 0$, $s > 0$.		\end{enumerate}	\end{enumerate}	\begin{proof}		First, we show that 2.\ implies 1. Let $\{N(t) : t \ge 0\}$ be a counting process that satisfies properties 2.(a), 2.(b), and 2.(c). Then the process also satisfies 1.(a) and 1(b) trivially. Let $t \ge 0$, and let $h > 0$ be given. By 2.(c), $N(t+h) - N(t)$ is Poisson-distributed with rate $\lambda h$, so		\begin{align*}			P(N(t+h) - N(t) = 1) &= \lambda h e^{-\lambda h} \\			&= \lambda h\left(1 - \lambda h + o(h)\right) \\			&= \lambda h + o(h),		\end{align*}		so the process satisfies 1.(c). Furthermore,		\begin{align*}			P(N(t+h)-N(t)\ge 2) &= 1 - \left[P(N(t+h)-N(t) = 0) + P(N(t+h) - N(t)) = 1)\right] \\			&= 1 - \left[e^{-\lambda h} + \lambda he^{-\lambda h}\right]\\			&= 1 - 1 + \lambda h + o(h) - \lambda h + o(h) \\			&= o(h)		\end{align*}		so the process satisfies 1.(d). Thus, 2.\ implies 1.				Second, we show that 1.\ implies 2. Let $\{N(t) : t \ge 0\}$ be a counting process that satisfies 1.(a), 1.(b), 1.(c), and 1.(d). Then the process trivially satisfies 2.(a) and 2.(b). Let $t \ge 0$ and $s > 0$ be given. Divide the interval $(t, t+s]$ into $n$ subintervals of equal length $h = \frac{s}{n}$ each. Let $M_i = N(t+ h) - N(t +(i-1)h)$ for $i = 1, 2, \dots, n$ be the number of events in each subinterval. Using a telescoping sum, we have 		\begin{equation*}			N(t+s) - N(t) = \sum_{i=1}^{n}M_i.		\end{equation*}		Let $\mathcal{S}$ be the set of all sequences of length $n$ whose entries are nonnegative integers that sum to $ k \ge 0$. Then		\begin{align*}			P(N(t+s) - N(t)=k) &= P\left(\sum_{i=1}^n M_i=k\right) \\[0.5em]			&= \sum_{(m_1,m_2,\dots, m_n) \in \mathcal{S}} P(M_1 = m_1, M_2=m_2, \dots, M_n=m_n).		\end{align*}		It follows from independence of increments that $M_1=m_1$, $M_2=m_2$, \dots $M_n = m_n$ are independent, so		\begin{equation*}			P(N(t+s) - N(t)=k) = \sum_{(m_1,m_2,\dots, m_n) \in \mathcal{S}} P(M_1 = m_1)P(M_2=m_2) \cdots P(M_n=m_n).		\end{equation*}		Let $\mathcal{S}'$ be the subset of $\mathcal{S}$ such that for $(m_1, m_2, \dots, m_n) \in \mathcal{S}'$ we have $m_i \le 1$ for $i =1,2,\dots, n$. We observe that $\mathcal{S}'$ contains $\binom{n}{k}$ elements because each $m_i$ in an element of $\mathcal{S}'$ must be either 0 or 1, and there must be exactly $k$ that are 1 in order for the sum of the $m_i$ to be $k$. Furthermore, by 1.(c) and 1.(d), we have		\begin{equation*}			P(M_i=0) = 1 - P(M_i =1) - P(M_i\ge 2) =1 - \lambda h + o(h), \qquad P(M_i = 1) = \lambda h + o(h).		\end{equation*}		It follows that		\begin{align*}		P(N(t+s) - N(t)=k) &= \sum_{(m_1, \dots, m_n) \in \mathcal{S}'}P(M_1=m_1)\cdots P(M_n=m_n) + \sum_{(m_1, \dots, m_n) \in \mathcal{S} \setminus \mathcal{S}'}P(M_1=m_1)\cdots P(M_n=m_n) \\		&= \binom{n}{k}\left(\lambda h + o(h)\right)^k\left(1 - \lambda h + o(h)\right)^{n-k} + R_n,		\end{align*}		where		\begin{equation*}			R_n = \sum_{(m_1, \dots, m_n) \in \mathcal{S} \setminus\mathcal{S}'}P(M_1=m_1)\cdots P(M_n=m_n).			\end{equation*}		Replacing $h$ by $\frac{s}{n}$, we have		\begin{equation*}			P(N(t+s) - N(t) = k) = \binom{n}{k}\left(\frac{\lambda s}{n} + o\left(\frac{1}{n}\right)\right)^k\left(1 - \frac{\lambda s}{n} + o\left(\frac{1}{n}\right)\right)^{n-k} + R_n.		\end{equation*}		We now take the limit as $n \to \infty$ on both sides. We begin by showing that $R_n\to 0$ as $n \to \infty$. Let $\mathcal{S}_{p,q}$ denote the set of all sequences $(m_1, \dots, m_n) \in \mathcal{S} \setminus \mathcal{S}'$ such that $p = \#\{i : m_i \ge 2\}$ and $q = \#\{i : m_i = 1\}$. We note that the values of $p$ and $q$ for which $\mathcal{S}_{p,q}$ is nonempty are determined by $p \ge 1$ and $2p + q \le k$. In particular, the number of nonempty sets $\mathcal{S}_{p,q}$ is independent of $n$.				Now we compute $\#\mathcal{S}_{p,q}$. There are $\binom{n}{p+q}$ ways to select the nonzero $m_i$. For each such choice there are furthermore $\binom{p+q}{p}$ ways to choose which $m_i$ are greater than 1. Lastly, we can think about choosing the values of the nonzero $m_i$ as distributing $k$ items among the $p+q$ slots so that $q$ slots have 1 item each and the remaining $p$ slots have 2 or more items each. Thus, we need to distribute $2p+q$ items to meet these requirements -- 2 for each of the $p$ slots and 1 for each of the $q$ slots -- then we can decide freely how to distribute the remaining $k-2p-q$ items among the $p$ slots that may have 2 or more items. There are $\binom{k-2p-q+p-1}{p-1}$ ways to select the $k-2p-q$ slots from among the $p$ choices with replacement allowed, where order does not matter. Hence,		\begin{align*}			\#\mathcal{S}_{p,q} &= \binom{n}{p+q}\binom{p+q}{p}\binom{k-p-q-1}{p-1} \\[0.5em]			&= \frac{n!}{(n-p-q)!(p+q)!}\binom{p+q}{p}\binom{k-p-q-1}{p-1} \\[0.5em]			&= \frac{n(n-1)\cdots(n-p-q+1)}{(p+q)!}\binom{p+q}{p}\binom{k-p-q-1}{p-1}\\			&= O(n^{p+q}),		\end{align*}		as the values of $k, p, q$ are independent of $n$. Writing		\begin{equation*}			R_n = \sum_{\substack{p \ge 1,\\ 2p+q\le k}}\sum_{(m_1,\dots, m_n)\in \mathcal{S}_{p,q}}P(M_1=m_1)\cdots P(M_n = m_n ),		\end{equation*}		we observe that each term in the sum of $\mathcal{S}_{p,q}$ has $q$ factors of $\frac{\lambda s}{n} + o\left(\frac{1}{n}\right)$ by 1.(c), and $p$ factors of $o\left(\frac{1}{n}\right)$ by 1.(d). The remaining factors are probabilities and, therefore, are bounded above by $1$. Hence,		\begin{align*}			\sum_{(m_1, \dots, m_n) \in \mathcal{S}_{p,q}}P(M_1 = m_1) \cdots P(M_n = m_n) &\le \#\mathcal{S}_{p,q}\left(\frac{\lambda s}{n} + o\left(\frac{1}{n}\right)\right)^q\left(o\left(\frac{1}{n}\right)\right)^p  \\			&\le O\left(n^{p+q}\right) \left(O\left(\frac{1}{n}\right)\right)^q\left(o\left(\frac{1}{n}\right)\right)^p \\			&=  O\left(n^{p+q}\right)o\left(\frac{1}{n^{p+q}}\right) \\			&= o(1)		\end{align*}		if $p \ge 1$. Since there are a fixed number of terms (independent of $n$) in the sum over $p \ge1$ and $2p+q \le k$, it follows that $R_n = o(1)$ as $n \to \infty$. In other words, $R_n \to 0$ as $n \to \infty$.				Now we handle the first term, starting with the right factor. There exists a sequence $\{a_n\}$ such that $a_n \to 0$ as $n \to \infty$, and		\begin{equation*}			\left(1 - \frac{\lambda s}{n} + o\left(\frac{1}{n}\right)\right)^n =\left(1 - \frac{\lambda s}{n} + \frac{a_n}{n}\right)^n.		\end{equation*}		Then		\begin{equation*}			\left(1 - \frac{\lambda s}{n} + o\left(\frac{1}{n}\right)\right)^n = \left(1 - \frac{\lambda s}{n}\right)^n + \sum_{i=1}^n \frac{n^{\underline{i}}}{i!}\left(1- \frac{\lambda s}{n}\right)^{n-i}\frac{a_n^i}{n^i},		\end{equation*}		where $n^{\underline{i}} = n(n-1)\cdots (n-i+1)$ is the falling factorial of $n$. Clearly, $\frac{n^{\underline{i}}}{n^i} \le 1$, and, for $n$ large enough, $\left(1-\frac{\lambda s}{n}\right)^{n-i} \le 1$. Let $a = \sup\limits_n |a_n| < \infty$. Given $\varepsilon > 0$, there exists $N$ such that		\begin{equation*}			\left|\sum_{i=N}^n \frac{a_n^i}{i!}\right| \le \sum_{i=N}^\infty \frac{a^i}{i!} < \frac{\varepsilon}{2}.		\end{equation*}		On the other hand, we must have		\begin{equation*}			\sum_{i=1}^N\frac{a_n^i}{i!} \to 0 \quad\text{as}\;n\to\infty		\end{equation*}		because $a_n^i \to 0$ as $n \to \infty$ for each of the finitely-many terms in the sum. Then for $n$ sufficiently large,		\begin{equation*}			\left|\sum_{i=1}^n\frac{n^{\underline{i}}}{i!}\left(1-\frac{\lambda s}{n}\right)^{n-i}\frac{a_n^i}{n^i}\right| \le \sum_{i=1}^N \frac{a_n^i}{i!} + \sum_{i=N}^\infty \frac{a^i}{i!} < \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon.		\end{equation*}		Thus,		\begin{equation*}			\lim_{n\to \infty}\left(1 - \frac{\lambda s}{n} + o\left(\frac{1}{n}\right)\right)^n = \lim_{n\to\infty}\left(1-\frac{\lambda s}{n}\right)^n = e^{-\lambda s}.		\end{equation*}		Lastly, we observe that		\begin{equation*}			\left(1-\frac{\lambda s}{n} + o\left(\frac{1}{n}\right)\right)^{-k} \to 1		\end{equation*}		as $n \to \infty$ because of the continuity of $x \mapsto x^{-k}$ at $1$. This handles the right factor:		\begin{equation*}			\lim_{n\to\infty}\left(1-\frac{\lambda s}{n} + o\left(\frac{1}{n}\right)\right)^{n-k} = e^{-\lambda s}.		\end{equation*}		The left factor is a bit simpler. We have		\begin{equation*}			\binom{n}{k}\left(\frac{\lambda s}{n} + o\left(\frac{1}{n}\right)\right)^k = \frac{n^{\underline{k}}(\lambda s)^k}{n^kk!} + \sum_{i=1}^k \frac{n^{\underline{k}}}{k!}\binom{k}{i}\frac{(\lambda s)^i}{n^i}\left(o\left(\frac{1}{n}\right)\right)^{k-i} =\frac{n^{\underline{k}}(\lambda s)^k}{n^kk!} + o\left(\frac{n^{\underline{k}}}{n^k}\right).		\end{equation*}		Since $n^{\underline{k}}$ is a polynomial of degree $k$ in $n$ with leading coefficient 1, it follows that $\frac{n^{\underline{k}}}{n^k} \to 1$ as $n \to \infty$. This completes the argument for the left factor:		\begin{equation*}			\lim_{n\to\infty}\binom{n}{k}\left(\frac{\lambda s}{n} + o\left(\frac{1}{n}\right)\right)^k = \frac{(\lambda s)^k}{k!}.		\end{equation*}		This completes the proof, as we finally obtain		\begin{equation*}			P(N(t+s) - N(t) = k) = \frac{(\lambda s)^ke^{-\lambda s}}{k!},		\end{equation*}		which implies that $N(t+s) - N(t)$ is Poisson-distributed with mean $\lambda s$, which proves 2.(c).	\end{proof}			\question*{Textbook: 36, p. 370}	\begin{alphaparts}		\questionpart Using the independence of the $X_i$ from each other and from $N(t)$ and the fact that $E[X_i] = \frac{1}{\mu}$ because $X_i$ is exponential with rate $\mu$, we have		\begin{align*}			E[S(t)] &= E\left[S(0)\prod_{i=1}^{N(t)}X_i\right] = sE\left[E\left[\prod_{i=1}^{N(t)}X_i\bigg\vert N(t)\right]\right] \\			&= s\sum_{k=0}^\infty E\left[\prod_{i=1}^n X_i\right]P(N(t)=n) \\			&= s\sum_{k=0}^\infty \frac{(\lambda t)^ne^{-\lambda t}}{n!}\prod_{i=1}^nE[X_i] \\			&= s\sum_{k=0}^\infty \frac{(\lambda t)^ne^{-\lambda t}}{n!}\frac{1}{\mu^n} \\			&= se^{\frac{\lambda t}{\mu} - \lambda t}\sum_{k=0}^\infty \left(\frac{\lambda t}{\mu}\right)^n\frac{e^{-\frac{\lambda t}{\mu}}}{n!} \\			&= se^{\lambda t \left(\frac{1}{\mu} - 1\right)}.		\end{align*}				\questionpart Note that		\begin{equation*}			S^2(t) = s^2\prod_{i=1}^{N(t)}X_i^2,		\end{equation*}		so by nearly the same calculation as in part (a) and the fact that $E[X_i^2] = \frac{2}{\mu^2}$ because $X_i$ is exponential with rate $\mu$, we have		\begin{align*}			E[S^2(t)] &= s^2\sum_{k=0}^\infty \frac{(\lambda t)^ne^{-\lambda t}}{n!}\prod_{i=1}^nE[X_i^2] \\			&= s^2e^{\frac{2\lambda t}{\mu^2} - \lambda t}\sum_{k=0}^\infty \left(\frac{2\lambda t}{\mu^2}\right)^n \frac{e^{-\frac{2\lambda t}{\mu^2}}}{n!} \\			&= s^2e^{\lambda t\left(\frac{2}{\mu^2}-1\right)}.		\end{align*}	\end{alphaparts}		\question*{Textbook: 58, p. 374}	We note that $\{N(t) : t\ge 0\}$, with $N(t) = N_1(t) + N_2(t)$, is a Poisson process with rate $\lambda_1 + \lambda_2$, and by our superposition theorem the $P(T_i = j) = \frac{\lambda_j}{\lambda_1 + \lambda_2}$, where $T_i$ is the type of the $i$th event in the process $\{N(t) : t \ge 0\}$.		Let $V_i$ be the amount of the $i$th claim in the process $\{N(t) : t \ge 0\}$. Then, by Bayes' Law,	\begin{equation*}		P(T_i = 1 |V_i=4000) = \frac{f_{V_i|T_i}(4000|1)P(T_i=1)}{f_{V_i}(4000)},	\end{equation*}	where $f_{V_i|T_i}(v|t)$ is the conditional PDF of $V_i$ given $T_i$, and $f_{V_i}(v)$ is the PDF of $V_i$. Since $V_i$ is exponential with mean 1000 given $T_i = 1$ and $V_i$ is exponential with mean 5000 given $T_i = 2$, it follows that	\begin{equation*}		f_{V_i|T_i}(v|1) = \frac{1}{1000}e^{-\frac{1}{1000}v}, \qquad v \ge 0,	\end{equation*}	and	\begin{equation*}		f_{V_i|T_i}(v|2) = \frac{1}{5000}e^{-\frac{1}{5000}v}, \qquad v \ge 0.	\end{equation*}	By the law of total probability,	\begin{align*}		f_{V_i}(v) &= f_{V_i|T_i}(v|1)P(T_i=1) + f_{V_i|T_i}(v|2)P(T_i=2) \\		&= \frac{1}{1000}e^{-\frac{1}{1000}v}\frac{\lambda_1}{\lambda_1 + \lambda_2} + \frac{1}{5000}e^{-\frac{1}{5000}v}\frac{\lambda_2}{\lambda_1 + \lambda_2} \\		&= \frac{1}{1100}e^{-\frac{1}{1000}v} + \frac{1}{50000}e^{-\frac{1}{5000}v}.	\end{align*}	Hence,	\begin{align*}		P(T_i=1|V_i=4000) &= \frac{\frac{1}{1000}e^{-\frac{4000}{1000}}\cdot\frac{10}{11}}{\frac{1}{1100}e^{-\frac{4000}{1000}} + \frac{1}{50000}e^{-\frac{4000}{5000}}} \\		&= \frac{\frac{1}{11}e^{-4}}{\frac{1}{11}e^{-4} + \frac{1}{500}e^{-\frac{4}{5}}} \\		&= \frac{1}{1 + \frac{11}{500}e^{\frac{16}{5}}} \\		&\approx 0.6495.	\end{align*}		\question*{Textbook: 64, p. 375}	\begin{alphaparts}		\questionpart Given $N(t) = n>0$, we have		\begin{equation*}			X = \sum_{i=1}^n (t - S_i),		\end{equation*}		where $S_i$ is the time of the $i$th arrival. If $N(t) = 0$, then $X = 0$. Then		\begin{equation*}			E[X | N(t)] = E\left[\sum_{i=1}^{N(t)}(t-S_i) \bigg\vert N(t)\right] = tN(t) - \sum_{i=1}^{N(t)}E[S_i |N(t)].		\end{equation*}		Given $N(t)$, the arrival times $S_1, \dots, S_{N(t)}$ are distributed the same as the order statistics $U_{(1)}, \dots, U_{(N(t))}$ of i.i.d. random variables $U_1, \dots, U_{N(t)}$ that are uniformly distributed on $[0,t]$. Thus, since the order statistics are a permutation of the original variables,		\begin{equation*}			E[X |N(t)] = tN(t) - \sum_{i=1}^{N(t)}E[U_{(i)}] = tN(t) - \sum_{i=1}^{N(t)}E[U_i] = tN(t) - \frac{t}{2}N(t) = \frac{tN(t)}{2}.		\end{equation*}				\questionpart Using the formula for $X$ given $N(t)$ from above,		\begin{equation*}			\Var[X|N(t)] = \Var\left[\sum_{i=1}^{N(t)}(t-S_i) \bigg\vert N(t)\right] = \Var[tN(t) | N(t)] + \Var\left[\sum_{i=1}^{N(t)}S_i \bigg\vert N(t)\right].		\end{equation*}		Since $\Var[tN(t) |N(t)] = 0$, and the $S_i$ given $N(t)$ have the same distribution as the order statistics from part (a), we have		\begin{equation*}			\Var[X|N(t)] = \sum_{i=1}^{N(t)}\Var\left[U_{(i)}\right].		\end{equation*} 		Once again, the order statistics are just a permutation of the original variables, so their sum is the same as the sum of the originals, giving		\begin{equation*}			\Var[X|N(t)] = \sum_{i=1}^{N(t)}\Var\left[U_i\right] = N(t)\frac{t^2}{12}.		\end{equation*}				\questionpart We compute $\Var[X]$ by conditioning on $N(t)$:		\begin{equation*}			\Var[X] = E[\Var[X |N(t)]] + \Var[E[X|N(t)]] = E\left[\frac{t^2N(t)}{12}\right] + \Var\left[\frac{tN(t)}{2}\right] = \frac{\lambda t^3}{12} + \frac{\lambda t^3}{4} = \frac{\lambda t^3}{3}.		\end{equation*}	\end{alphaparts}		\question*{Textbook: 80, p. 378}	\begin{alphaparts}		\questionpart The formula is true for $n = 1$ because		\begin{equation*}			F(t_1) = P(S_1 \le t_1) = 1 - P(S_1 > t_1) = 1 - P(N(t_1) = 0) = 1 - e^{-m(t_1)}, \qquad t_1 > 0,		\end{equation*}		is the CDF of $S_1$, so the PDF of $S_1$ is		\begin{equation*}			f_{S_1}(t_1) = F'(t_1) = e^{-m(t_1)}m'(t_1) = e^{-m(t_1)}\lambda(t_1), \qquad t_1 > 0,		\end{equation*}		as $m'(t) = \lambda(t)$ by definition. 				Suppose for induction that the formula holds for some $n \ge 1$. Let $t_1, \dots, t_{n+1}$ be given, and let $h_1,\dots, h_{n+1} > 0$ be given. Define the event $A_i = \{t_i \le S_i \le t_i + h_i\}$. 				If $t_{i+1} \le t_i$ for some $i=1,\dots, n$, then		\begin{equation*}			P\left(\bigcap_{i=1}^{n+1}A_i\right) \le P(A_i \cap A_{i+1}) = P(t_i \le S_i \le t_i + h_i,\, t_{i+1}\le S_{i+1}\le t_{i+1}+h_{i+1}).		\end{equation*}		Since $t_i \le S_i \le t_i+h_i$ and $t_{i+1} \le S_{i+1} \le t_{i+1} + h_{i+1}$ implies that two events ($i$ and $i+1$) occur in the interval $[t_{i+1}, t_{i+1}+h_{i+1}]$, it follows that		\begin{equation*}			P\left(\bigcap_{i=1}^{n+1}A_i\right)\le o(h_{i+1}).		\end{equation*}		This implies that the joint PDF of $S_1, \dots, S_{n+1}$ vanishes if $t_{i+1} \le t_i$. Thus, we consider only the case that $t_i < t_{i+1}$ for $i=1,\dots, n$. Furthermore, if $t_1 < 0$, then for sufficiently small $h_1$ the event $A_1$ would require $S_1 < 0$, which is impossible, so the PDF must vanish also if $t_1 < 0$, and we need only consider the case $t_1 > 0$.				We observe that		\begin{align*}			P\left(\bigcap_{i=1}^{n+1}A_i\right) &= P\left(A_{n+1} \cap \bigcap_{i=1}^n A_i\right) = P\left(A_{n+1} \bigg\vert \bigcap_{i=1}^{n}A_i\right)P\left(\bigcap_{i=1}^nA_i\right) \\			&= P\left(A_{n+1}\bigg\vert \bigcap_{i=1}^nA_i\right)(f_{S_1,\dots,S_n}(t_1,\dots,t_n)h_1h_2\dots,h_n + o(|h|)),		\end{align*}		where $h = (h_1,\dots, h_n)^T$. For $h_n$ small enough that $t_n+h_n < t_{n+1}$, and $h_{n-1}$ small enough that $t_{n-1} < t_{n-1} + h_{n-1}$, the event $A_{n+1}$ given the events $A_i, \dots A_n$ is equivalent to the event that no events occur between $t_n+h_n$ and $t_{n+1}$ and that 2 or more events do not occur between $t_n$ and $t_n + h_n$ and that at least one event occurs between $t_{n+1}$ and $t_{n+1} + h_{n+1}$. That is,		\begin{align*}			P\left(\bigcap_{i=1}^{n+1}A_i\right) ={} &P(N(t_n+h_n)-N(t_n)\le 1, N(t_{n+1} - t_n-h_n) - N(t_n+h_n) = 0, N(t_{n+1}+h_{n+1}) - N(t_{n+1}) \ge 1)\\ &{}\cdot (f_{S_1,\dots,S_n}(t_1,\dots,t_n)h_1\dots h_n + o(|h|)).		\end{align*}		Since $h_n$ and $h_{n+1}$ are sufficiently small, the independence of increments property implies that		\begin{align*}			P\left(\bigcap_{i=1}^{n+1}A_i\right) = {} &P(N(t_n+h_n) - N(t_n)\le 1)P(N(t_{n+1} - t_n-h_n) - N(t_n+h_n) = 0)P(N(t_{n+1} +h_{n+1}) - N(t_{n+1}) \ge 1)\\ {}&\cdot (f_{S_1,\dots S_n}(t_1, \dots  t_n)h_1\dots h_n + o(|h|)).		\end{align*}		From (one) definition of an inhomogeneous Poisson process, we have 		\begin{equation*}			P(N(t_n+h_n) - N(t_n) \le 1) = 1 - P(N(t_n + h_n) - N(t_n) \ge 2) = 1 - o(h_n).		\end{equation*}		Thus,		\begin{align*}			P\left(\bigcap_{i=1}^{n+1}A_i\right) = {}&(1-o(h_n))P(N(t_{n+1} - t_n -h_n) - N(t_n+h_n)=0)(1-P(N(t_{n+1} + h_{n+1}) - N(t_{n+1}) = 0))\\&{}\cdot (f_{S_1,\dots,S_n}(t_1,\dots, t_n)h_1\dots h_n + + o(|h|)).		\end{align*}		Additionally,		\begin{equation*}			P(N(t_{n+1} - t_n - h_n) - N(t_n + h_n) =0) = e^{-\int_{t_n+h_n}^{t_{n+1}}\lambda(s)\dee s},		\end{equation*}		and		\begin{equation*}			P(N(t_{n+1} + h_{n+1}) - N(t_{n+1}) = 0) = e^{-\int_{t_{n+1}}^{t_{n+1}+h_{n+1}}\lambda(s)\dee s}.		\end{equation*}		It follows that		\begin{equation*}			P\left(\bigcap_{i=1}^{n+1}A_i\right) = (1-o(h_n))e^{-\int_{t_n+h_n}^{t_{n+1}}\lambda(s)\dee s}\left(1-e^{\int_{t_{n+1}}^{t_{n+1}+h_{n+1}}\lambda(s)\dee s}\right)(f_{S_1,\dots,S_n}(t_1,\dots, t_n)h_1\dots h_n + o(|h|)).		\end{equation*}		Then		\begin{alignat*}{2}			f_{S_1,\dots, S_n, S_{n+1}}(t_1,\dots, t_n, t_{n+1}) &= \lim_{(h_1,\dots, h_{n+1})\to 0}{}&& \frac{P\left(\bigcap_{i=1}^{n+1}A_i\right)}{h_1h_2\dots h_nh_{n+1}}	\\			&=\lim_{(h_1\dots, h_{n+1})\to 0}{} && (1-o(h_n))e^{-\int_{t_n+h_n}^{t_{n+1}}\lambda(s)\dee s}\frac{1-e^{-\int_{t_{n+1}}^{t_{n+1}+h_{n+1}}\lambda(s)\dee s}}{h_{n+1}}\\ &&&{}\cdot \frac{f_{S_1,\dots,S_n}(t_1,\dots, t_n)h_1\dots h_n + o(|h|)}{h_1\dots h_n} \\			&= \lim_{(h_1\dots, h_{n+1})\to 0}{} &&e^{-\int_{t_n}^{t_{n+1}}\lambda(s)\dee s}f_{S_1,\dots S_n}(t_1,\dots, t_n)\frac{1-e^{-\int_{t_{n+1}}^{t_{n+1}+h_{n+1}}\lambda(s)\dee s}}{h_{n+1}}.		\end{alignat*}		Using L'Hopital's rule, we get		\begin{align*}			f_{S_1,\dots,S_{n+1}}(t_1,\dots, t_{n+1}) &= f_{S_1,\dots, S_n}(t_1,\dots, t_n)e^{-\int_{t_n}^{t_{n+1}}\lambda(s)\dee s} \lim_{h_{n+1}\to 0}\frac{1 - e^{-\int_{t_{n+1}}^{t_{n+1}+h_{n+1}}\lambda(s)\dee s}}{h_{n+1}} \\			&= \lambda(t_1)\dots \lambda(t_n)e^{-\int_{0}^{t_n}\lambda(s)\dee s - \int_{t_{n}}^{t_{n+1}}\lambda(s)\dee s} \lim_{h_{n+1}\to 0}\lambda(t_{n+1} + h_{n+1})e^{-\int_{t_{n+1}}^{t_{n+1}+h_{n+1}}\lambda(s)\dee s} \\			&= \lambda(t_1)\dots \lambda(t_n)\lambda(t_{n+1})e^{-\int_{0}^{t_{n+1}}\lambda (s)\dee s} \\			&= \lambda(t_1)\dots \lambda(t_n)\lambda(t_{n+1})e^{-m(t_{n+1})}.		\end{align*}		Thus, the formula for the joint PDF of $S_1, \dots, S_{n+1}$ holds. This means that it holds for all $n$ by induction.				\questionpart Using a similar argument to that in the book, we note that for $0 < s_1 < \dots < s_n < t$, the event that $S_1 = s_1$, $S_2 = s_2$, \dots $S_n = s_n$ and $N(t) = n$ is equivalent to the event that $T_1 = s_1$, $T_2 = s_2 - s_1$, \dots, $T_n = s_n - s_{n-1}$, $T_{n+1} > t - s_n$, where $T_i$ in our case is the amount of time until the next event occurs starting at time $t = s_{i-1}$, with $s_0 = 0$.				Since the density of $T_i$ at $s_i - s_{i-1}$ is given by $e^{-\int_{s_{i-1}}^{s_i}\lambda(s)\dee s} = \lambda(s_i)e^{-m(s_i) + m(s_{i-1})}$, and 		\begin{equation*}			P(T_{n+1} > t - s_n) = 1 - P(T_{n+1} \le t-s_{n}) = e^{-\int_{s_n}^t \lambda(s)\dee s} = e^{-m(t)+m(s_n)},		\end{equation*}		we have		\begin{align*}			f_{S_1,\dots,S_n|N(t)}(s_1,\dots, s_n|n) &= \frac{\lambda(s_1)e^{-m(s_1) + m(s_0)} \cdots \lambda(s_n)e^{-m(s_n)+m(s_{n-1})}e^{-\int_{s_n}^t \lambda(s)\dee s}}{P(N(t)=n)} \\			&= \frac{\lambda(s_1)\cdots\lambda(s_n)e^{-m(t)}n!}{e^{m(t)}(m(t))^n} \\			&= \frac{n!\lambda(s_1)\cdots\lambda(s_n)}{(m(t))^n}, \qquad 0 < s_1 < \dots < s_n < t.		\end{align*}				\questionpart Let $X_1, \dots, X_n$ be independent random variables with identical distribution given by the PDF		\begin{equation*}			f(s) = \frac{\lambda(s)}{m(t)}, \qquad s \in [0, t].		\end{equation*}		Then the joint PDF of the order statistics $X_{(1)}, \dots, X_{(n)}$ is given by		\begin{equation*}			f(s_1, \dots, s_n) = n!f(s_1)\cdots f(s_n) = \frac{n!\lambda(s_1)\cdots \lambda(s_n)}{(m(t))^n}, \qquad 0 < s_1 < \dots < s_n < t.		\end{equation*}		Thus, the distribution of $S_1, \dots, S_n$ given $N(t) = n$ is the same as the distribution of the order statistics $X_{(1)}, \dots, X_{(n)}$.	\end{alphaparts}		\question*{Textbook: 88, p. 379}	Let $X_n$ the amount withdrawn on the $n$th withdrawal. Then the total daily withdrawal is	\begin{equation*}		Y = \sum_{n=1}^{N(15)} X_n.	\end{equation*}	To find $P(Y < 6000)$, we condition on $N(15)$:	\begin{align*}		P(Y < 6000) &= \sum_{n=1}^\infty P(Y < 6000 | N(15)=n) P(N(15)=n) \\		&= \sum_{n=1}^\infty P(Y<6000 |N(15)=n) \frac{180^ne^{-180}}{n!}.	\end{align*}	Given $N(15)=n$, the Central Limit Theorem means that $Z = (Y/n - E[X_n]) / \sqrt{\Var[X_n]/n}$ roughly follows the standard normal distribution, if $n$ is sufficiently large. Then	\begin{equation*}		P(Y < 6000|N(15)=n) = P\left(Z <  \frac{6000/n - 30}{50/\sqrt{n}} \bigg\vert N(15)=n \right) \approx \Phi\left(120/\sqrt{n} - 0.6\sqrt{n}\right),	\end{equation*}	where $\Phi$ is the CDF of the standard normal distribution. Let $F$ be the CDF of $N(15)$, which is Poisson-distributed with mean 180. Numerical calculation of $F$ using Python shows that	\begin{equation*}		F(70) < 10^{-18}, \qquad 1-F(320) < 10^{-18},	\end{equation*}	so if we sum only the terms from $n=70$ to $n=320$ in the infinite sum for $P(Y<6000)$, we will incur an error less than machine precision ($\sim 10^{-18}$). Furthermore, by restricting to $n \ge 70$, we will ensure that our normal approximation of $Y$ is fairly accurate. This gives	\begin{equation*}		P(Y<6000) \approx \sum_{n=70}^{320} \Phi(120/\sqrt{n} - 0.6\sqrt{n})\frac{180^ne^{-180}}{n!} \approx 0.7805.	\end{equation*}\end{document}