\documentclass{homework}\input{../../standardcmd.tex}\input{../homework_shared.tex}\usepackage{listings}\newcommand{\hwnum}{4}\renewcommand{\questiontype}{Problem}\newcommand{\fil}{\mathcal{F}}\begin{document}	\maketitle		\question We need to show that $E[|M_n|] < \infty$ for $n\ge 1$, that $\{M_n\}$ is adapted to $\{\fil_n\}$, and that $E[M_{n+1}|\fil_n] = M_n$ for $n \ge 1$.	\begin{enumerate}		\item We have		\begin{align*}			E[|M_n|] &= E\left[\left|S_n^2 - v_n\right|\right] = E\left[\left|\left(S_0 + \sum_{i=1}^n \xi_i\right)^2 - \sum_{i=1}^n \sigma_i^2\right|\right] \\			&\le E\left[\left|S_0 + \sum_{i=1}^n\xi_i^2\right|\right] + \sum_{i=1}^n\sigma_i^2 \\			&\le S_0^2 + \sum_{i=1}^nE[\xi_i^2] + \sum_{i=1}^n \sigma_i^2 <\infty.		\end{align*}				\item Since $M_n$ is a continuous function of $\xi_1, \dots, \xi_n$, and $\xi_i$ is $\fil_n$-predictable for $i =1,\dots, n$, it follows that $M_n$ is $\fil_n$-predictable. Thus, $\{M_n\}$ is adapted to $\{\fil_n\}$.				\item We have		\begin{align*}			E[M_{n+1}|\fil_n] &= E\left[\left(S_0 + \sum_{i=1}^{n+1}\xi_i\right)^2 - \sum_{i=1}^{n+1}\sigma_i^2\bigg|\fil_n\right] \\			&= E\left[\left(S_0 + \sum_{i=1}^n\xi_i\right)^2 + 2\left(S_0+\sum_{i=1}^n\xi_i\right)\xi_{n+1} + \xi_{n+1}^2\bigg|\fil_n\right] - \sum_{i=1}^{n+1}\sigma_i^2.		\end{align*}		Since $\left(S_0 + \sum\limits_{i=1}^n\xi_i\right)^2$ is a continuous function of $\xi_1,\dots, \xi_n$, it is $\fil_n$-predictable because $\xi_1,\dots,\xi_n$ are all $\fil_n$-predictable. Then		\begin{equation*}			E[M_{n+1}|\fil_n] = \left(S_0 + \sum_{i=1}^n\xi_i\right)^2 - \sum_{i=1}^n\sigma_i^2 + 2\left(S_0 + \sum_{i=1}^n\xi_i\right)E[\xi_{n+1}|\fil_n] + E[\xi_{n+1}^2|\fil_n] - \sigma_{n+1}^2.		\end{equation*}		Since $\xi_{n+1}$ is independent of $\fil_n$, it follows that $\xi_{n+1}^2$ is also independent of $\fil_n$, and $E[\xi_{n+1}|\fil_n] = E[\xi_{n+1}] = 0$, and $E[\xi_{n+1}^2|\fil_n] = E[\xi_{n+1}] = \sigma_{n+1}^2$. Therefore,		\begin{equation*}			E[M_{n+1}|\fil_n] = \left(S_0 + \sum_{i=1}^n\xi_i\right)^2 - \sum_{i=1}^n\sigma_i^2 = M_n.		\end{equation*}	\end{enumerate}		\question	\begin{alphaparts}		\questionpart Let $\fil_n = \sigma(X_1, \dots, X_n)$. Then $\{M_n\}$ is a martingale with respect to $\{\fil_n\}$. To show this, we need to show that $E[|M_n|] < \infty$ for $n\ge1$, that$\{M_n\}$ is adapted to $\{\fil_n\}$, and that $E[M_{n+1} |\fil_n] = M_n$.		\begin{enumerate}[label=\arabic*.]			\item We have			\begin{equation*}				E[|M_n|] = E[2^nX_n] = 2^n E[U_1\cdots U_n] = 2^n \prod_{i=1}^n E[U_i] = 1 < \infty			\end{equation*}			because $U_1,\dots, U_n$ are independent with $E[U_i] = \frac{1}{2}$ for $i = 1,2,\dots, n$.						\item Let $\fil_n = \sigma(X_1,\dots, X_n)$. Then $M_n$ is a continuous function of $X_1, \dots, X_n$, each of which is $\fil_n$-predictable, so $M_n$ is $\fil_n$-predictable.						\item We have			\begin{equation*}				E[M_{n+1}|\fil_n] = E[2^{n+1}X_{n+1}|\fil_n] = 2^{n+1}E[X_nU_n|\fil_n].			\end{equation*}			Since $U_n$ is independent of $\fil_n$ and $X_n$ is $\fil_n$-predictable, we have			\begin{equation*}				E[M_{n+1}|\fil_n] = 2^{n+1}X_nE[U_n|\fil_n] = 2^{n+1}X_nE[U_n] = 2^nX_n = M_n.			\end{equation*}		\end{enumerate}				\questionpart Note that $-\log(U_i)$ is exponentially-distributed with mean 1 because, by the CDF method,		\begin{equation*}			P(-\log(U_i) \le x) = P(U_i \le e^{-x}) = e^{-x}, \qquad x \ge 0.		\end{equation*}		Then $E[\log(U_i)]= -E[-\log(U_i)]= -1$, and, by the Strong Law of Large Numbers,		\begin{equation*}			\frac{1}{n}\log(X_n) = \frac{1}{n}\sum_{i=1}^n\log(U_i) \overset{\text{a.s.}}{\longrightarrow}-1		\end{equation*}		as $n \to \infty$.		\questionpart To show that $M_n \to 0$ almost surely, we must show that $P(B) = 1$, where $B = \{M_n \to 0\}$. Since $P(B) \le 1$ in any case, we only need to show that $P(B) \ge 1$. 				To this end, let $A = \{\frac{1}{n}M_n \to \log(2) -1\}$. Then $P(A) = 1$ because, by {\bf(b)},		\begin{equation*}			\frac{1}{n}\log(M_n) =  \log(2) + \frac{1}{n}X_n \overset{\text{a.s.}}{\longrightarrow} \log(2) - 1.		\end{equation*}				The natural exponent base $e$ is greater than $2$, so $\log(2) < 1$, and there exists $\delta > 0$ such that $\log(2) + \delta < 1$. Let $\omega \in A$, and let  $\varepsilon > 0$. Since $\omega \in A$, there exists $N_1>0$ such that 		\begin{equation*}			\frac{1}{n}\log(X_n(\omega)) < -\log(2) - \delta \quad\text{for all $n > N_1$.}		\end{equation*}		Choose $N > \max\left\{N_1, -\frac{\log(\varepsilon)}{\delta}\right\}$. Then for all $n > N$, we have		\begin{align*}			\frac{1}{n}\log(X_n(\omega)) < -\log(2)-\delta & \implies \log(X_n(\omega)) + n\log(2) < -n\delta \\			&\implies |M_n(\omega)| < e^{-n\delta} < e^{-N\delta} < \varepsilon.		\end{align*}		Thus, $M_n(\omega) \to 0$ as $n\to \infty$, so $\omega \in B$. Since $\omega \in A$ was arbitrary, it follows that $A \subseteq B$, and $1 = P(A) \le P(B)$. This proves that $M_n \to 0$ almost surely.	\end{alphaparts}		\question $T_1$ and $T_1 +1$ are stopping times, but $T_1 - 1$ is not.	\begin{enumerate}[label=\arabic*.]		\item $T_1$ is a stopping time because		\begin{equation*}			\{T_1 \le n\} = \bigcup_{i=1}^n \{M_i = 1\} \in \fil_n		\end{equation*}		because $\{M_i =1\} \in \fil_i \subseteq \fil_n$ for $i = 1,2,\dots, n$.				\item $T_1 +1$ is a stopping time because		\begin{equation*}			\{T_1 +1\le n\} = \{T_1 \le n-1\} = \bigcup_{i=1}^{n-1}\{M_i=1\}\in\fil_n		\end{equation*}		because $\{M_i=1\} \in \fil_i\subseteq \fil_n$ for $i=1,2\dots, n-1$.				\item $T_1 - 1$ is not necessarily a stopping time because		\begin{equation*}			\{T_1 - 1 \le n\} = \{T_1 \le n + 1\} = \{M_{n+1} = 1\}\cup \bigcup_{i=1}^n\{M_i=1\} = \{M_{n+1}=1\}\cup\{T_1\le n\}.		\end{equation*}		Since $\{M_{n+1} =1\}$ may not be in $\fil_n$, we cannot conclude that $\{T_1-1\le n\} \in \fil_n$.	\end{enumerate}		\question Let $A_s$ be the event that $X_t -1$ for all $t>s$, for $s=1,2,\dots$, that is,	\begin{equation*}		A_s = \bigcap_{t>s}\{X_t = -1\}.	\end{equation*}	For an outcome $\omega \in A_s$, it is clear that $M_t(\omega) \to -\infty$. Thus, $A_s \subseteq\{M_t \to -\infty\}$ for all $s =1,2,\dots$ Then $\{M_t\to-\infty\}^C \subseteq A_s^C$ for all $s=1,2,\dots$		By Boole's inequality, it follows that	\begin{equation*}		P\left(A_s^C\right) = P\left(\bigcup_{t> s}\{X_t = t^2-1\}\right) \le \sum_{t>s}P(X_t = t^2-1) = \sum_{t>s}\frac{1}{t^2}.	\end{equation*}	Then $P\left(A_s^C\right) \to 0$ as $s \to \infty$ because the last expression above is the tail of the convergent sum $\sum\limits_{t=1}^\infty\frac{1}{t^2}$. Hence, by the squeeze theorem,	\begin{equation*}		0 \le P(M_t\not\to-\infty) \le P(A_s^C) \implies P(M_t\not\to-\infty) = 0.	\end{equation*}	This means that $P(M_t\to -\infty) =1$, that is, $M_t \to -\infty$ almost surely.		\question Since $\{X_t\}$ is a sub-martingale, there is a filtration $\{\fil_t\}$ to which $\{X_t\}$ is adapted. 		\question To show that $\{Y_n\}$ is a martingale with respect to $\{\fil_n\}$, where $\fil_n = \sigma(X_1,\dots, X_n)$, we need to show that $E[|Y_n|] < \infty$ for all $n$, that $\{Y_n\}$ is adapted to $\{\fil_n\}$, and that $E[Y_{n+1}|\fil_n] = Y_n$.	\begin{enumerate}		\item We have			\begin{equation*}				E[|Y_n|] = E\left[\left|S_n^2 - \sum_{k=1}^nX_k^2\right|\right] \le E\left[\left(\sum_{k=1}^nX_k\right)^2\right] + \sum_{k=1}^nE[X_k^2] \le E\left[n^2\sum_{k=1}^nX_k^2\right] + n\sigma^2 \le (n^3+n)\sigma^2 < \infty.			\end{equation*}		because for any set of real numbers $\{a_i : i=1,2\dots,n\}$, we have		\begin{equation*}			\left(\sum_{i=1}^na_i\right)^2 \le \left(\sum_{i=1}^n |a_i|\right)^2 \le \left(n\max_{i=1,2,\dots,n}|a_i|\right)^2 = n^2\max_{i=1,2,\dots,n}a_i^2 \le n^2\sum_{i=1}^na_i^2.		\end{equation*}		\item It is easy to see that $Y_n$ is a continuous function of $X_1,X_2,\dots, X_n$, so $Y_n$ is adapted to $\fil_n = \sigma(X_1,X_2,\dots,X_n)$ for all $n$.		\item Since $X_{n+1}$ is independent of $X_1,\dots, X_n$, it follows that $X_{n+1}$ is independent of $\fil_n$. Thus,		\begin{equation*}			\begin{aligned}			E[Y_{n+1}|\fil_n] &= E\left[\left(\sum_{k=1}^{n+1}X_k\right)^2 - \sum_{k=1}^{n+1}X_k^2\Bigg|\fil_n\right] \\[0.5em]			&= E\left[\left(\sum_{k=1}^nX_k\right)^2 + 2X_{n+1}\sum_{k=1}^nX_k + X_{n+1}^2 - \sum_{k=1}^nX_k^2 - X_{n+1}^2\Bigg|\fil_n\right] \\[0.5em]			&= \left(\sum_{k=1}^nX_k\right)^2 - \sum_{k=1}^nX_k^2 + 2E[X_{n+1}|\fil_n]\sum_{k=1}^nX_k \\			&= Y_n + 2E[X_{n+1}]\sum_{k=1}^nX_k \\			&= Y_n.			\end{aligned}		\end{equation*}	\end{enumerate}		We have $S_n = \sum\limits_{k=1}^nX_k^2$. Since $\{X_k\}$ are independent, so, too, are $\{X_k^2\}$. Furthermore, we have $E[X_k^2] = \sigma^2$ for all $k$. Then, by Wald's Equation,	\begin{equation*}		E[S_\tau] = E[X_1^2]E[\tau] = \sigma^2E[\tau].	\end{equation*}		\question Let $Y_n = e^{2b(S_n-bn)}$. Then $\{Y_n\}$ is a martingale with respect to $\{\fil_n\}$, where $\fil_n = \sigma(X_1,\dots,X_n)$. To show this, we need to prove that $E[|Y_n|] < \infty$ for all $n$, that $\{Y_n\}$ is adapted to $\{\fil_n\}$, and that $E[Y_{n+1}|\fil_n] = Y_n$.	\begin{enumerate}		\item Since $X_k$ is standard normal for $k=1,2,\dots$, and $S_n = X_1+X_2+\dots + X_n$, it follows that $S_n$ is also normal with mean 0 and variance $n$, so that the PDF of $S_n$ is		\begin{equation*}			f(s) = \frac{1}{\sqrt{2\pi n}}e^{-\frac{1}{2n}s^2}.		\end{equation*}		Then		\begin{equation*}			E[|Y_n|] = E\left[e^{2b(S_n - bn)}\right] = \int_{-\infty}^\infty e^{2bs - 2bn} \frac{1}{\sqrt{2\pi n}}e^{-\frac{1}{2n}s^2}\;\text{d}s \lesssim \int_{-\infty}^\infty e^{-\frac{1}{2n}(s - 2nb)^2}\;\text{d}s < \infty.		\end{equation*}		\item $Y_n$ is a continuous function of $X_1,\dots,X_n$, so $Y_n$ is adapted to $\fil_n$ for all $n$.		\item Since $X_{n+1}$ is independent of $X_1,\dots, X_n$, it is also independent of $\fil_n$. Then		\begin{align*}			E[Y_{n+1}\big|\fil_n] &= E\left[e^{2b(S_{n+1} - b(n+1))}\big|\fil_n\right] = E\left[e^{2b(S_n + X_{n+1} - bn - b)}|\fil_n\right] \\[0.5em]			&= E\left[e^{2b(S_n-bn)}e^{2b(X_{n+1} -b)}\big|\fil_n\right] \\			&= Y_nE\left[e^{2b(X_{n+1}-b)}\right] \\			&= Y_n e^{-2b^2}\int_{-\infty}^\infty e^{2bx}\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2}\;\text{d}x \\			&= Y_n e^{-2b^2}\int_{-\infty}^\infty\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(x-2b)^2+2b^2}\;\text{d}x \\			&= Y_n\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2}\;\text{d}x \\			&= Y_n,		\end{align*}		where the second-to-last line follows from the change of variables $u = x-2b$, and the last line follows from the fact that the integrand is the PDF of a standard normal random variable.	\end{enumerate}		All martingales are submartingales, so $\{Y_n\}$ is also a submartingale. Additionally, $Y_n > 0$ for all $n$. Hence, by the Doob submartingale inequality,	\begin{equation*}		P\left(Y_n > e^{2bc}\right) \le P\left(\max_{1\le k\le n}Y_k > e^{2bc}\right) \le e^{-2bc}E[Y_n].	\end{equation*}	Since $Y_n$ is a martingale,	\begin{align*}		E[Y_n] &= E[Y_1] = E\left[e^{2b(X_1 - b)}\right] = e^{-2b^2}E\left[e^{2bX_1}\right] \\		&= e^{-2b^2}\int_{-\infty}^\infty e^{2bx}\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2}\;\text{d}x \\		&= e^{-2b^2}\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(x-2b)^2 + 2b^2}\;\text{d}x \\		&= \int_{-\infty}^\infty\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2}\;\text{d}x \\		&= 1.	\end{align*}	Furthermore,	\begin{equation*}		\{Y_n = e^{2bc}\} = \left\{e^{2b(S_n - bn)} > e^{2bc}\right\} = \{2b(S_n - bn) > 2bc\} = \{S_n > bn + c\};	\end{equation*}	therefore,	\begin{equation*}		P(S_n > bn +c) \le e^{-2bc}.	\end{equation*}		\question\end{document}