\documentclass{homework}
\input{../../standardcmd.tex}
\input{../homework_shared.tex}

\newcommand{\hwnum}{6}
\renewcommand{\questiontype}{Problem}


\begin{document}
	\maketitle
	
	\question Let $V$ be a vector space over $\mathbb{C}$. If $\lVert \cdot \rVert$ is a norm on $V$, then there exists an inner product $\langle \cdot, \cdot\rangle$ on $V$ that induces $\lVert \cdot \rVert$ if and only if
	\begin{equation}
		\label{eq:parallelogram}
		\lVert \vec{x} + \vec{y} \rVert^2 + \lVert \vec{x} - \vec{y}\rVert^2 = 2(\lVert \vec{x}\rVert^2 + \lVert \vec{y}\rVert^2) \quad \text{for all $\vec{x}, \vec{y}\in V$.}
	\end{equation}
	
	\begin{proof}
		Suppose that there is an inner product $\langle\cdot, \cdot\rangle$ that induces $\lVert\cdot\rVert$. Then for all $\vec{x},\vec{y}\in V$,
		\begin{equation*}
			\begin{aligned}
				\lVert \vec{x} +\vec{y}\rVert^2 + \lVert \vec{x} - \vec{y}\rVert^2 &= \langle\vec{x}+\vec{y},\vec{x}+ \vec{y}\rangle + \langle\vec{x}-\vec{y},\vec{x}-\vec{y}\rangle  \\
				&= \langle\vec{x},\vec{x}\rangle  + \langle\vec{x},\vec{y}\rangle + \langle\vec{y},\vec{x}\rangle +  \langle\vec{y},\vec{y}\rangle + \langle\vec{x},\vec{x}\rangle +\langle\vec{x},-\vec{y}\rangle + \langle-\vec{y},\vec{x}\rangle + \langle-\vec{y},-\vec{y}\rangle \\
				&= 2\langle\vec{x},\vec{x}\rangle + 2\langle\vec{y},\vec{y}\rangle + \langle\vec{x},\vec{y}\rangle + \langle\vec{y},\vec{x}\rangle - \langle\vec{x},\vec{y}\rangle - \langle\vec{y},\vec{x}\rangle \\
				&= 2(\lVert\vec{x}\rVert^2 + \lVert\vec{y}\rVert^2)
			\end{aligned}
		\end{equation*}
		because $\lVert\cdot\rVert$ is induced by $\langle\cdot,\cdot\rangle$.
		
		Conversely, suppose that \eqref{eq:parallelogram} holds. We note that $V$ is also a vector space over $\mathbb{R}$ if we use the same addition operator and a scalar multiplication operator given by restricting the scalar multiplication from $\mathbb{C}$ to $\mathbb{R}$. This can be verified by checking the vector space axioms. The axioms relating only to the addition operator are automatically satisfied because we are using the same addition. Then the axioms relating to the scalar multiplication remain. Let $a,b \in \mathbb{R}$, and $\vec{x}, \vec{y} \in V$.
		\begin{enumerate}
			\item \textbf{(Closure)} Since $a \in \mathbb{C}$, it follows that $a\vec{x} \in V$.
			\item \textbf{(Associativity of field and scalar multiplication)} Since $a, b \in \mathbb{R}$, we also have $ab \in \mathbb{R}$. On the other hand, $a,b,ab \in \mathbb{C}$, so $a(b\vec{x}) = (ab)\vec{x}$.
			\item \textbf{(Multiplicative identity)} We note that $1 \in \mathbb{R}$, and $1\vec{x} = \vec{x}$.
			\item \textbf{(Distributivity over vector addition)} Since $a \in \mathbb{C}$, we have $a(\vec{x} + \vec{y}) = a\vec{x} + a\vec{y}$.
			\item \textbf{(Distributivity over field addition)} Since $a, b \in \mathbb{R}$, we also have $a+b\in\mathbb{R}$. On the other hand, $a,b,a+b\in\mathbb{C}$, so $(a+b)\vec{x} = a\vec{x} + b\vec{x}$.
		\end{enumerate}
		Furthermore, we also see that $\lVert\cdot\rVert$ is a norm for $V$ as a vector space over $\mathbb{R}$. In particular, positive definiteness is retained because it does not depend on the scalar multiplication, and the triangle inequality is retained because it depends only on the vector addition operator, which is the same. For the homogeneity property, we note that if $a \in \mathbb{R}$, and $\vec{x} \in V$, then $a \in \mathbb{C}$, so
		\begin{equation*}
			\lVert a\vec{x}\rVert = |a|\lVert\vec{x}\rVert.
		\end{equation*}
		Thus, we can apply the theorem we proved in class; namely, the function $\langle\cdot,\cdot\rangle_R$ defined by
		\begin{equation*}
			\langle\vec{x},\vec{y}\rangle_R = \frac{1}{4}\left(\lVert\vec{x} + \vec{y}\rVert^2 - \lVert\vec{x} - \vec{y}\rVert^2\right), \qquad \vec{x},\vec{y}\in V,
		\end{equation*}
		is an inner product for $V$, as a vector space over $\mathbb{R}$, that induces $\lVert\cdot\rVert$.
		
		Now define $\langle\cdot,\cdot\rangle$ by
		\begin{equation*}
			\langle\vec{x},\vec{y}\rangle = \langle\vec{x},\vec{y}\rangle_R+ i\langle i\vec{x}, \vec{y}\rangle_R, \qquad \vec{x},\vec{y}\in V.
		\end{equation*}
		Then $\langle\cdot,\cdot\rangle$ is an inner product for $V$, as a vector space over $\mathbb{C}$, that induces $\lVert\cdot\rVert$.
		
		Let $\vec{x}, \vec{y}, \vec{z}\in V$. To begin with, we observe the property
		\begin{equation*}
			\langle i\vec{x},\vec{y}\rangle_R = \frac{1}{4}\left(\lVert i\vec{x} + \vec{y}\rVert - \lVert i\vec{x} - \vec{y}\rVert^2\right) = \frac{1}{4}\left(\lVert \vec{x} - i\vec{y}\rVert^2 - \lVert \vec{x} + i\vec{y}\rVert^2\right) = -\langle \vec{x}, i\vec{y}\rangle_R.
		\end{equation*}
		Thus,
		\begin{equation*}
			\langle \vec{x}, \vec{x}\rangle = \langle\vec{x}, \vec{x}\rangle_R + i\langle i\vec{x},\vec{x}\rangle_R = \lVert \vec{x}\rVert^2
		\end{equation*}
		because $\langle\vec{x},\vec{x}\rangle_R = \lVert \vec{x}\rVert^2$, and $\langle i\vec{x}, \vec{x}\rangle_R = \langle \vec{x},i\vec{x}\rangle_R$ by symmetry, but also $\langle i\vec{x}, \vec{x}\rangle_R = -\langle \vec{x}, i\vec{x} \rangle_R$ by the above property, which implies that $\langle i\vec{x},\vec{x}\rangle_R = 0$.
		This proves that $\langle\cdot,\cdot\rangle$ is positive definite because $\lVert \cdot\rVert$ is positive definite, and it also shows that $\langle\cdot,\cdot\rangle$ induces $\lVert\cdot\rVert$, as long as $\langle\cdot,\cdot\rangle$ is actually an inner product.
		
		Since
		\begin{equation*}
			\langle \vec{x}, \vec{y}\rangle= \langle\vec{x},\vec{y}\rangle_R + i\langle i\vec{x},\vec{y}\rangle_R = \langle\vec{y},\vec{x}\rangle_R -i\langle\vec{x}, i\vec{y}\rangle_R = \langle \vec{y},\vec{x}\rangle_R - i\langle i\vec{y}, \vec{x}\rangle_R = \overline{\langle\vec{y}, \vec{x}\rangle},
		\end{equation*}
		we see that $\langle\cdot,\cdot\rangle$ is conjugate symmetric. Next, we see that
		\begin{equation*}
			\langle \vec{x}, \vec{y}+\vec{z}\rangle = \langle\vec{x}, \vec{y}+\vec{z}\rangle_R + i\langle i\vec{x}, \vec{y}+\vec{z}\rangle_R = \langle \vec{x},\vec{y}\rangle_R + \langle\vec{x}, \vec{z}\rangle_R + i(\langle i\vec{x}, \vec{y}\rangle_R + \langle i\vec{x},\vec{z}\rangle_R) = \langle\vec{x},\vec{y}\rangle + \langle\vec{x}, \vec{z}\rangle.
		\end{equation*}
		Lastly, let $c = a+ ib \in \mathbb{C}$, with $a,b \in \mathbb{R}$. Then
		\begin{equation*}
		\begin{aligned}
			\langle \vec{x}, c\vec{y}\rangle &= \langle\vec{x}, a\vec{y} + ib\vec{y}\rangle_R + i\langle i\vec{x}, a\vec{y} + ib\vec{y}\rangle_R \\
			&= a\langle\vec{x}, \vec{y}\rangle_R + b\langle\vec{x},i\vec{y}\rangle_R +ia\langle i\vec{x},\vec{y}\rangle_R + ib\langle i\vec{x},i\vec{y}\rangle_R \\
			&= a\langle\vec{x},\vec{y}\rangle_R - b\langle i\x, \y\rangle_R + ia\langle i\x, \y\rangle_R -ib\langle \x,-\y\rangle_R \\
			&= (a+ib)\langle \x,\y\rangle_R + (ia - b)\langle i\x,\y\rangle_R \\
			&= (a+ib)\langle \x, \y\rangle_R + i(a + ib)\langle i\x,\y\rangle_R \\
			&= c\langle \x,\y\rangle.
		\end{aligned}
		\end{equation*}
		Thus, $\langle\cdot,\cdot\rangle$ is positive definite, conjugate symmetric, and linear in the second argument. Then $\langle\cdot,\cdot\rangle$ defines an inner product on $V$ that induces the norm $\lVert\cdot\rVert$.
	\end{proof}
	
	\question Let $\vec{x}_1,\vec{x}_2, \dots,\vec{x}_n\in\mathbb{R}^n$ be orthonormal. Let $A \in \mathbb{R}^n$. If $A\vec{x}_1, A\vec{x}_2,\dots, A\vec{x}_n$ are also orthonormal, then $A$ is orthogonal.
	
	\begin{proof}
		Let $X = \mat{\vec{x}_1 & \vec{x}_2 & \cdots & \vec{x}_n}$, and let $B = \mat{A\vec{x}_1 & A\vec{x}_2 & \cdots & A\vec{x}_n}$. Then $B = AX$. Since the columns of $B$ and $X$ are orthonormal, they are both orthogonal matrices. Therefore,
		\begin{equation*}
			A = BX^T.
		\end{equation*}
		Since $(BX^T)^T(BX^T) = XB^TBX^T = I$ and $(BX^T)(BX^T)^T = BX^TXB^T = I$ by the orthogonality of $B$ and $X$, it follows that $A$ is invertible with $A^{-1} = (BX^T)^T = A^T$. This implies that $A$ is orthogonal.
	\end{proof}
	
	\question The algorithm for the Gram-Schmidt process is described in Algorithm \ref{alg:gs}. An implementation in Python is provided in Listing \ref{lst:gs}. We note that this implementation detects linear dependence of the columns of $A$ as a part of the Gram-Schmidt process by checking if the produced orthogonal vectors are zero (well, almost zero, to account for numerical rounding error). This is possible because the columns of $A$ are linearly dependent if and only if the Gram-Schmidt process produces a zero vector at some point. This is easy to prove.
	
	For $j < i$, each $\vec{b}_j$ is a linear combination of the first $j$ columns of $A$. We can prove this by induction. For the base case, $\vec{b}_1 = \lVert\vec{a}_1\rVert^{-1}\vec{a}_1$. For some $1 \le k < i-1$, suppose for induction that $\vec{b}_j = \sum\limits_{m=1}^jc_{jm}\vec{a}_m$ for $1 \le j \le k$ and some constants $c_{jm}$. Then
	\begin{equation*}
		\vec{b}_{k+1} = \vec{a}_{k+1} - \sum_{p=1}^k \langle\vec{b}_p,\vec{a}_{k+1}\rangle \sum_{m=1}^p c_{pm}\vec{a}_m
	\end{equation*}
	which completes the proof by induction.
	
	Suppose that $\vec{b}_i = \vec{0}$. Then
	\begin{equation*}
		\vec{0} = \vec{b}_i = \vec{a}_i - \sum_{p=1}^{i-1}\langle\vec{b}_p, \vec{a}_i\rangle \sum_{m=1}^pc_{pm}\vec{a}_m.
	\end{equation*}
	The coefficient of $\vec{a}_i$ is non-zero, so a non-trivial linear combination of the columns of $A$ is $\vec{0}$, meaning that the columns of $A$ are linearly dependent.
	
	Conversely, if the columns of $A$ are linearly dependent, then there exists $c_1,\dots, c_m$ not all equal to zero such that
	\begin{equation*}
		\sum_{i=1}^m c_i\vec{a}_i = \vec{0}.
	\end{equation*}
	Let $k$ be the largest integer such that $c_k \ne 0$. Then
	\begin{equation*}
		\vec{0} = \sum_{i=1}^kc_i\vec{a}_i = c_k\vec{b}_k  + \sum_{p=1}^{k-1}\langle\vec{b}_p,\vec{a}_k\rangle\vec{b}_p + \sum_{i=1}^{k-1}\left(c_i\vec{b}_i + c_i\sum_{p=1}^{i-1}\langle\vec{b}_p,\vec{a}_i\rangle\vec{b}_p\right).
	\end{equation*}
	The coefficient of $\vec{b}_k$ is nonzero, so it follows that the columns of $B$ are linearly dependent. Since the columns of $B$ are also orthogonal because of the Gram-Schmidt process, one of them must be zero.
	
	The command \texttt{python -m gs} can be used to run the tests, which verify that the function works across a range of input types that cover every code path. The output from running these tests is given in Listing \ref{lst:gs_output}.
	\begin{algorithm}
		\caption{Gram-Schmidt Orthogonalization}\label{alg:gs}
		\KwIn{Matrix $A \in \mathbb{R}^{n\times m}$ with linearly independent columns $\vec{a}_1, \dots, \vec{a}_m \in \mathbb{R}^n$}
		\KwOut{Matrix $B \in \mathbb{R}^{n\times m}$, whose columns $\vec{b}_1, \dots, \vec{b}_m \in \mathbb{R}^n$ are the orthogonal vectors obtained by applying the Gram-Schmidt process to the columns of $A$}
		$c \gets 1$\;
		\Repeat{$c = n$}{
			$\vec{b}_c \gets \vec{a}_c - \sum\limits_{p=1}^{c-1} \langle \vec{b}_p, \vec{a}_c\rangle\vec{b}_p$ \tcp*{Sum is 0 by convention if $c=1$}
			$\vec{b}_c \gets \frac{\vec{b}_c}{\lVert\vec{b}_c\rVert}$\;
		}
	\end{algorithm}
	
	\pythonexternal[caption={Python implementation of the Gram-Schmidt process}, label={lst:gs}]{gs.py}
	
	\txtexternal[caption={Output for test cases}, label={lst:gs_output}]{gs_output.txt}
	
\end{document}